{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ispDL1G4Fc"
      },
      "source": [
        "# Chapter 5 - Connecting Causality and Deep Learning\n",
        "\n",
        "The notebook is a code companion to chapter 5 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/altdeep/causalML/blob/master/book/chapter%205/chapter_5_Connecting_Causality_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook was written in Google Colab using Python version 3.10.12. The versions of the main libraries include:\n",
        "* pyro version 1.84\n",
        "* torch version 2.2.1\n",
        "* pandas version 2.0.3\n",
        "* torchvision vserions 0.18.0+cu121\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVk-jq9hxOYo"
      },
      "source": [
        "Pgmpy allows us to fit conventional Bayesian networks on a causal DAG. However, with modern deep probabilistic machine learning frameworks like pyro, we can build more nuanced and powerful causal models.  In this tutorial, we fit a variational autoencoder on a causal DAG that represents a dataset that mixes handwritten MNIST digits and typed T-MNIST images.\n",
        "\n",
        "![TMNIST-MNIST](https://github.com/altdeep/causalML/blob/master/book/chapter%205/images/MNIST-TMNIST.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_gEnU8rWN3e",
        "outputId": "ddeb6bd9-07ae-4db7-ca56-9c1d2d6a5603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyro-ppl==1.8.4 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (1.8.4)\n",
            "Requirement already satisfied: numpy>=1.7 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from pyro-ppl==1.8.4) (2.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from pyro-ppl==1.8.4) (3.4.0)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from pyro-ppl==1.8.4) (0.1.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from pyro-ppl==1.8.4) (2.9.1)\n",
            "Requirement already satisfied: tqdm>=4.36 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from pyro-ppl==1.8.4) (4.67.1)\n",
            "Requirement already satisfied: filelock in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch>=1.11.0->pyro-ppl==1.8.4) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.11.0->pyro-ppl==1.8.4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->pyro-ppl==1.8.4) (3.0.3)\n",
            "Requirement already satisfied: torchvision in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torchvision) (2.4.0)\n",
            "Requirement already satisfied: torch==2.9.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torchvision) (2.9.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torchvision) (12.1.0)\n",
            "Requirement already satisfied: filelock in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from torch==2.9.1->torchvision) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages (from jinja2->torch==2.9.1->torchvision) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyro-ppl==1.8.4\n",
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsu5QxOW0aAx"
      },
      "source": [
        "## Listing 5.1: Setup for GPU training\n",
        "\n",
        "The code will run faster if we use CUDA, if it's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "lfWTRtPoWF5H"
      },
      "outputs": [],
      "source": [
        "import torch    #A\n",
        "USE_CUDA = True    #A\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    #A\n",
        "#A Use CUDA if it is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UoHQdJpG4Fe"
      },
      "source": [
        "## Listing 5.2: Combining the data\n",
        "\n",
        "First, we create a Dataset object that will combine our two datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_GoP_qMKWF5I"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "\n",
        "class CombinedDataset(Dataset):    #A\n",
        "    def __init__(self, csv_file):\n",
        "        self.dataset = pd.read_csv(csv_file)\n",
        "        print(self.dataset.head())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images = self.dataset.iloc[idx, 3:]    #B\n",
        "        images = np.array(images, dtype='float32')/255.  #B\n",
        "        images = images.reshape(28, 28)    #B\n",
        "        transform = transforms.ToTensor()    #B\n",
        "        images = transform(images)    #B\n",
        "        digits = self.dataset.iloc[idx, 2]    #C\n",
        "        digits = np.array([digits], dtype='int')    #C\n",
        "        is_handwritten = self.dataset.iloc[idx, 1]    #D\n",
        "        is_handwritten = np.array([is_handwritten], dtype='float32')    #D\n",
        "        return images, digits, is_handwritten    #E\n",
        "\n",
        "#A This class loads and processes a dataset that combines the MNIST and Typeface MNIST. The output is a torch.utils.data.Dataset object.\n",
        "#B Load, normalize, and reshape the images to a 28x28 pixel.\n",
        "#C Get and process the digits labels, 0-9.\n",
        "#D 1 for handwritten digits (MNIST) 0 for “typed’ digits (TMNIST).\n",
        "#E Return tuple of the image, the digit label, and the is_handwritten label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UGXmWzuJlpL"
      },
      "source": [
        "## Listing 5.3: Downloading, splitting and loading the data\n",
        "\n",
        "Next, we'll download the data and create the combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "S4jn_XvTJlxH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):    #A\n",
        "    combined_dataset = CombinedDataset(\n",
        "\"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
        "    )\n",
        "    n = len(combined_dataset)    #B\n",
        "    train_size = int(0.8 * n)    #B\n",
        "    test_size = n - train_size    #B\n",
        "    train_dataset, test_dataset = random_split(    #B\n",
        "        combined_dataset,    #B\n",
        "        [train_size, test_size],    #B\n",
        "        generator=torch.Generator().manual_seed(42)    #B\n",
        "    )    #B\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    train_loader = DataLoader(    #C\n",
        "        train_dataset,    #C\n",
        "        batch_size=batch_size,    #C\n",
        "        shuffle=True,    #C\n",
        "        **kwargs    #C\n",
        "    )    #C\n",
        "    test_loader = DataLoader(    #C\n",
        "        test_dataset,    #C\n",
        "        batch_size=batch_size,    #C\n",
        "        shuffle=True,    #C\n",
        "        **kwargs    #C\n",
        "    )    #C\n",
        "    return train_loader, test_loader\n",
        "#A Setup data loader that loads the data and splits it into training and test sets\n",
        "#B Allot 80% of the data to training data, the remaining 20% to test data.\n",
        "#C Create training and test loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   index  is_mnist  labels  1  2  3  4  5  6  7  ...  775  776  777  778  779  \\\n",
            "0      0         0       0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "1      1         0       1  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "2      2         0       2  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "3      3         0       3  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "4      4         0       4  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "\n",
            "   780  781  782  783  784  \n",
            "0    0    0    0    0    0  \n",
            "1    0    0    0    0    0  \n",
            "2    0    0    0    0    0  \n",
            "3    0    0    0    0    0  \n",
            "4    0    0    0    0    0  \n",
            "\n",
            "[5 rows x 787 columns]\n"
          ]
        }
      ],
      "source": [
        "train_loader, test_loader = setup_dataloaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "column 0 is simply index of the datapoint in the dataset, column 1 is is_handwritten, column 2 is digits, columns 3: are image values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRXUclDPpKpd"
      },
      "source": [
        "## Listing 5.4: Implement the decoder\n",
        "\n",
        "First, we specify a decoder. The decoder maps the latent variable Z, a variable representing the value of the digit, and a binary variable representing whether the digit is handwritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "pQEwj1mh-TTj"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class Decoder(nn.Module):    #A\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28    #B\n",
        "        digit_dim = 10    #C\n",
        "        is_handwritten_dim = 1    #D\n",
        "        self.softplus = nn.Softplus()    #E\n",
        "        self.sigmoid = nn.Sigmoid()    #E\n",
        "        encoding_dim = z_dim + digit_dim + is_handwritten_dim    #F\n",
        "        self.fc1 = nn.Linear(encoding_dim, hidden_dim)    #F\n",
        "        self.fc2 = nn.Linear(hidden_dim, img_dim)    #G\n",
        "\n",
        "    def forward(self, z, digit, is_handwritten):    #H\n",
        "        input = torch.cat([z, digit, is_handwritten], dim=1)   #I\n",
        "        hidden = self.softplus(self.fc1(input))    #J\n",
        "        img_param = self.sigmoid(self.fc2(hidden))    #K\n",
        "        return img_param\n",
        "#A The decoder method of a VAE class.\n",
        "#B Image is 28 by 28 pixels\n",
        "#C Digit is one-hot encoded digits 0-9, i.e., a vector of length 10.\n",
        "#D An indicator for if the digit is handwritten that has size 1\n",
        "#E The softplus and sigmoid are nonlinear transforms (activation functions) used in mapping between layers.\n",
        "#F fc1 is a linear function that maps Z vector, the digit, and the is_handwritten to a linear out, which is passed through a softplus activation function to create a \"hidden layer\" - a vector whose length is given by hidden_layer.\n",
        "#G The fc2 linearly maps the hidden layer to an output passed to a sigmoid function. The resulting value is a value between 0 and 1.\n",
        "#H Define the forward computation from the latent Z variable value to a generated X variable value.\n",
        "#I First combine Z and the labels.\n",
        "#J Then compute the hidden layer.\n",
        "#K Finally, pass the hidden layer to a linear transform, then to a sigmoid transform to output a parameter vector of length 784. Each element of the vector corresponds to a Bernoulli parameter value for an image pixel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYlhi-IlpshP"
      },
      "source": [
        "## Listing 5.5: The causal model\n",
        "\n",
        "The `model` method implements the causal model. First it samples the latent variable Z, the digit variable, and the is_handwritten variable. These are passed to the decoder, which generates the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ndMbxPUfJlqu"
      },
      "outputs": [],
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "dist.enable_validation(False)    #A\n",
        "def model(self, data_size=1):    #B\n",
        "    pyro.module(\"decoder\", self.decoder)    #B\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    z_loc = torch.zeros(data_size, self.z_dim, **options)    #C\n",
        "    z_scale = torch.ones(data_size, self.z_dim, **options)    #C\n",
        "    # print(z_loc.shape, z_scale.shape)\n",
        "    z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))    #C - we are defining the prior belief that latent styles are normally distributed around origin\n",
        "    # print(z.shape) # shape = (batch_size, z_dim)\n",
        "    p_digit = torch.ones(data_size, 10, **options)/10    #D - categorical RV - prior probability distribution for the 10 digits\n",
        "    digit = pyro.sample(    #D\n",
        "        \"digit\",    #D\n",
        "        dist.OneHotCategorical(p_digit)    #D\n",
        "    )    #D\n",
        "    p_is_handwritten = torch.ones(data_size, 1, **options)/2    #E - bernoulli RV - prior probability for MNIST vs TMNIST\n",
        "    is_handwritten = pyro.sample(    #E\n",
        "        \"is_handwritten\",    #E\n",
        "        dist.Bernoulli(p_is_handwritten).to_event(1)    #E\n",
        "    )    #E\n",
        "    img_param = self.decoder(z, digit, is_handwritten)    #F - Each element of the vector corresponds to a Bernoulli parameter value for an image pixel.\n",
        "    img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))  #G - samples to get a realization from the img_param distribution\n",
        "    return img, digit, is_handwritten\n",
        "#A Disabling distribution validation lets Pyro calculate loglikelihoods for pixels even though the pixels are not binary values.\n",
        "#B The model of a single image. Within the method we register the decoder, a PyTorch module, with Pyro. This lets Pyro know about the parameters inside of the decoder network.\n",
        "#C We model the joint probability of Z, digit, and is_handwritten sampling each from canonical distributions. We sample Z from a multivariate normal with location parameter z_loc (all zeros) and scale parameter z_scale (all ones).\n",
        "#D We also sample the digit from a one-hot categorical distribution. Equal probability is assigned to each digit.\n",
        "#E We similarly sample the is_handwritten variable from a Bernoulli.\n",
        "#F The decoder maps digit, is_handwritten, and Z to a probability parameter vector.\n",
        "#G That parameter vector is passed to the Bernoulli distribution, which models the pixel values in the data. The pixels are not technically Bernoulli binary variables, but we'll relax this assumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xn77L8qCmn"
      },
      "source": [
        "## Listing 5.6 Method for applying model to N images in data\n",
        "\n",
        "`training_model` extends `model` towards representing each image in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "RQGgpOPSWOKa"
      },
      "outputs": [],
      "source": [
        "def training_model(self, img, digit, is_handwritten, batch_size):    #A\n",
        "    conditioned_on_data = pyro.condition(    #B\n",
        "        self.model,\n",
        "        data={\n",
        "            \"digit\": digit,\n",
        "            \"is_handwritten\": is_handwritten,\n",
        "            \"img\": img\n",
        "        }\n",
        "    )\n",
        "    with pyro.plate(\"data\", batch_size):    #C\n",
        "        img, digit, is_handwritten = conditioned_on_data(batch_size)\n",
        "    return img, digit, is_handwritten\n",
        "#A The model represents the data generating process for one image. The training_model applies that model to the N images in the training data.\n",
        "#B Now we condition the model on the evidence in the training data.\n",
        "#C This context manager represents the N-size plate representing repeating IID examples in the data in Figure 5.9. In this case, N is the batch size. It works like a for loop iterating over each data unit in the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia-xBsT6qioA"
      },
      "source": [
        "## Listing 5.7: Implement the encoder\n",
        "\n",
        "The encoder takes an image, the digit, and whether the variable is handwritten, and infers the latent representation Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UGGxsLWIWkvG"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):    #A\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28    #B\n",
        "        digit_dim = 10    #C\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()    #D\n",
        "        input_dim = img_dim + digit_dim + is_handwritten_dim    #E\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)    #E\n",
        "        self.fc21 = nn.Linear(hidden_dim, z_dim)    #F\n",
        "        self.fc22 = nn.Linear(hidden_dim, z_dim)    #F\n",
        "\n",
        "    def forward(self, img, digit, is_handwritten):    #G\n",
        "        input = torch.cat([img, digit, is_handwritten], dim=1)    #H\n",
        "        hidden = self.softplus(self.fc1(input))    #I\n",
        "        z_loc = self.fc21(hidden)    #J\n",
        "        z_scale = torch.exp(self.fc22(hidden))    #J\n",
        "        return z_loc, z_scale\n",
        "#A The encoder is an instance of a Pytorch module.\n",
        "#B The input image is 28X28 = 784 pixels.\n",
        "#C The digit dimension is 10.\n",
        "#D In the encoder, we’ll only use the softplus transform (activation function).\n",
        "#E The linear transform fc1 combines with the softplus to map the 784 dimensional pixel vector, 10 dimensional digit label vector, and 2 dimensional is_handwritten vector to the hidden layer.\n",
        "#F The linear transforms fc21 and fc22 will combine with the softplus to map the hidden vector to Z’s vector space.\n",
        "#G Define the reverse computation from an observed X variable value to a latent Z variable value.\n",
        "#H Combine the image vector, digit label, and is-handwritten label into one input.\n",
        "#I Map the input to the hidden layer.\n",
        "#J The VAE framework will sample Z from a Normal distribution that approximates P(Z|img, digit, is_handwritten). The final transforms map the hidden layer to a location and scale parameter for that Normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP920mmcqtsS"
      },
      "source": [
        "## Listing 5.8: The guide function\n",
        "\n",
        "`training_guide` contains the encoder. The purpose of `training_guide` is to approximate P(Z|image, digit, is_handwritten) during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "xy2T3-CtZp5d"
      },
      "outputs": [],
      "source": [
        "def training_guide(self, img, digit, is_handwritten, batch_size):    #A\n",
        "    pyro.module(\"encoder\", self.encoder)    #B\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    with pyro.plate(\"data\", batch_size):    #C\n",
        "        z_loc, z_scale = self.encoder(img, digit, is_handwritten)    #D\n",
        "        normal_dist = dist.Normal(z_loc, z_scale).to_event(1)    #D\n",
        "        z = pyro.sample(\"Z\", normal_dist)    #E\n",
        "#A training_guide is a method of the VAE which will use the encoder.\n",
        "#B Register the encoder so Pyro is aware of its weight parameters.\n",
        "#C This is the same plate context manager for iterating over the batch data that we see in the training_model.\n",
        "#D Use the encoder to map an image and its labels to parameters of a Normal distribution.\n",
        "#E Sample Z from that Normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbit-PBlrayq"
      },
      "source": [
        "## Listing 5.9: The full VAE code\n",
        "\n",
        "Now we implement all the parts in the VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "o5vOr1GXe_3P"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        z_dim=50,    #A\n",
        "        hidden_dim=400,    #B\n",
        "        use_cuda=USE_CUDA,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.setup_networks()\n",
        "\n",
        "    def setup_networks(self):    #C\n",
        "        self.encoder = Encoder(self.z_dim, self.hidden_dim)\n",
        "        self.decoder = Decoder(self.z_dim, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    model = model    #D\n",
        "    training_model = training_model    #D\n",
        "    training_guide = training_guide    #D\n",
        "\n",
        "#A Setting the latent dimension to have a dimension of 50.\n",
        "#B Setting the hidden layers to have a dimension of 400.\n",
        "#C Setup the encoder and decoder.\n",
        "#D Adding in the methods for model, training_model, and training_guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8_IhBKK0kpO"
      },
      "source": [
        "## Listing 5.10 Helper function for plotting images\n",
        "\n",
        "The following utility functions helps us visualize progress during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "ohxiEjB30lHa"
      },
      "outputs": [],
      "source": [
        "def plot_image(img, title=None):    #A\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "#A Helper function for plotting an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTFLSyVW06tr"
      },
      "source": [
        "## Listing 5.11: Define a helper functions for reconstructing and viewing the images\n",
        "\n",
        "These additional utility functions help us selected and reshape images, as well as generate new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "QqBkhFAy066G"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reconstruct_img(vae, img, digit, is_hw, use_cuda=USE_CUDA):    #A\n",
        "    img = img.reshape(-1, 28 * 28)\n",
        "    digit = F.one_hot(torch.tensor(digit), 10)\n",
        "    is_hw = torch.tensor(is_hw).unsqueeze(0)\n",
        "    if use_cuda:\n",
        "        img = img.cuda()\n",
        "        digit = digit.cuda()\n",
        "        is_hw = is_hw.cuda()\n",
        "    z_loc, z_scale = vae.encoder(img, digit, is_hw)\n",
        "    z = dist.Normal(z_loc, z_scale).sample()\n",
        "    img_expectation = vae.decoder(z, digit, is_hw)\n",
        "    return img_expectation.squeeze().view(28, 28).detach()\n",
        "\n",
        "def compare_images(img1, img2):    #B\n",
        "    fig = plt.figure()\n",
        "    ax0 = fig.add_subplot(121)\n",
        "    plt.imshow(img1.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.title('original')\n",
        "    ax1 = fig.add_subplot(122)\n",
        "    plt.imshow(img2.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.title('reconstruction')\n",
        "    plt.show()\n",
        "#A Given an input image, \"reconstructs\" the image by passing through the encoder then through the decoder.\n",
        "#B Plots two images side by side for comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79MHVV7313mT"
      },
      "source": [
        "## Listing 5.12: Data processing helper functions for training\n",
        "\n",
        "Next, we'll create some helper functions for handling the data. We'll use `get_random_example` to grab random images from the dataset. `reshape_data` will convert an image and its labels into input for the encoder. We'll use `generate_data` and `generate_coded_data` will simulate an image from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "fmSgUQrl1tFN"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_random_example(loader):    #A\n",
        "    random_idx = np.random.randint(0, len(loader.dataset))    #A\n",
        "    img, digit, is_handwritten = loader.dataset[random_idx]    #A\n",
        "    return img.squeeze(), digit, is_handwritten    #A\n",
        "\n",
        "def reshape_data(img, digit, is_handwritten):    #B\n",
        "    digit = F.one_hot(digit, 10).squeeze()    #B\n",
        "    img = img.reshape(-1, 28*28)    #B\n",
        "    return img, digit, is_handwritten    #B\n",
        "\n",
        "def generate_coded_data(vae, use_cuda=USE_CUDA):    #C\n",
        "    z_loc = torch.zeros(1, vae.z_dim)    #C\n",
        "    z_scale = torch.ones(1, vae.z_dim)    #C\n",
        "    z = dist.Normal(z_loc, z_scale).to_event(1).sample()    #C\n",
        "    p_digit = torch.ones(1, 10)/10    #C\n",
        "    digit = dist.OneHotCategorical(p_digit).sample()    #C\n",
        "    p_is_handwritten = torch.ones(1, 1)/2    #C\n",
        "    is_handwritten = dist.Bernoulli(p_is_handwritten).sample()    #C\n",
        "    if use_cuda:    #C\n",
        "        z = z.cuda()\n",
        "        digit = digit.cuda()\n",
        "        is_handwritten = is_handwritten.cuda()    #C\n",
        "    img = vae.decoder(z, digit, is_handwritten)    #C\n",
        "    return img, digit, is_handwritten    #C\n",
        "\n",
        "def generate_data(vae, use_cuda=USE_CUDA):    #D\n",
        "    img, digit, is_handwritten = generate_coded_data(vae, use_cuda)    #D\n",
        "    img = img.squeeze().view(28, 28).detach()    #D\n",
        "    digit = torch.argmax(digit, 1)    #D\n",
        "    is_handwritten = torch.argmax(is_handwritten, 1)    #D\n",
        "    return img, digit, is_handwritten    #D\n",
        "#A Chose a random example from the dataset.\n",
        "#B Reshape the data.\n",
        "#C Generate data that is encoded.\n",
        "#D Generate (unencoded) data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFp3sIBu2-eW"
      },
      "source": [
        "## Listing 5.13: Set up the training procedure\n",
        "\n",
        "Next we set up traing. The training objective `Trace_ELBO` simultaneously trains the parameters of the encoder and the decoder. It focuses on minimizing reconstruction error (how much information is lost when an image encoded, and then decoded once again) and [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the distribution modeled by the guide (the variational distribution) and the P(Z|image, is_handwritten, digit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "iEbyt3uo3Fhm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   index  is_mnist  labels  1  2  3  4  5  6  7  ...  775  776  777  778  779  \\\n",
            "0      0         0       0  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "1      1         0       1  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "2      2         0       2  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "3      3         0       3  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "4      4         0       4  0  0  0  0  0  0  0  ...    0    0    0    0    0   \n",
            "\n",
            "   780  781  782  783  784  \n",
            "0    0    0    0    0    0  \n",
            "1    0    0    0    0    0  \n",
            "2    0    0    0    0    0  \n",
            "3    0    0    0    0    0  \n",
            "4    0    0    0    0    0  \n",
            "\n",
            "[5 rows x 787 columns]\n"
          ]
        }
      ],
      "source": [
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "pyro.clear_param_store()    #A\n",
        "vae = VAE()    #B\n",
        "train_loader, test_loader = setup_dataloaders(batch_size=256)    #C\n",
        "svi_adam = Adam({\"lr\": 1.0e-3})    #D\n",
        "model = vae.training_model    #E\n",
        "guide = vae.training_guide    #E\n",
        "svi = SVI(model, guide, svi_adam, loss=Trace_ELBO())    #E\n",
        "#A Clear any values of the parameters in the guide memory.\n",
        "#B Initalize the VAE\n",
        "#C Load the data\n",
        "#D Initialize the optizer\n",
        "#E Initialize the SVI loss calculator. Loss negative \"expected lower bound\" (ELBO)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klKFXtLpTOos"
      },
      "source": [
        "## Listing 5.14: Setting up a test evaluation procedure\n",
        "\n",
        "When training generative models, it is useful to setup a procedure that uses test data to evaluate how well training is progressing. You can include anything you think is useful to monitor during training. Here, I calculate and print the loss function on the test data, just to make sure test loss is progressively decreasing along with training loss (a flattening of test loss while training loss continued to decrease would indicate overfitting).\n",
        "\n",
        "But a more direct signal at how well our model is training is to generate and view images. In my test evaluation procedure, I produce two visualizations. First, I inspect how well it can reconstruct a random image from the test data. I pass the image through the encoder then through the decoder, creating a “reconstruction” of the image. Then I plot the original and reconstructed image side-by-side and compare them visually, looking to see that they are close to identical.\n",
        "\n",
        "Next, I visualize how well it is performing as an overall generative model by generating and plotting an image from scratch. I run this code once each time a certain number of epochs are run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "XSviOogCTN2d"
      },
      "outputs": [],
      "source": [
        "def test_epoch(vae, test_loader):\n",
        "    epoch_loss_test = 0    #A\n",
        "    for img, digit, is_hw in test_loader:    #A\n",
        "        batch_size = img.shape[0]    #A\n",
        "        if USE_CUDA:    #A\n",
        "            img = img.cuda()    #A\n",
        "            digit = digit.cuda()    #A\n",
        "            is_hw = is_hw.cuda()    #A\n",
        "        img, digit, is_hw = reshape_data(    #A\n",
        "            img, digit, is_hw    #A\n",
        "        )    #A\n",
        "        epoch_loss_test += svi.evaluate_loss(    #A\n",
        "            img, digit, is_hw, batch_size    #A\n",
        "        )    #A\n",
        "    test_size = len(test_loader.dataset)    #A\n",
        "    avg_loss = epoch_loss_test/test_size    #A\n",
        "    print(\"Epoch: {} avg. test loss: {}\".format(epoch, avg_loss))    #A\n",
        "    print(\"Comparing a random test image to its reconstruction:\")    #B\n",
        "    random_example = get_random_example(test_loader)    #B\n",
        "    img_r, digit_r, is_hw_r = random_example    #B\n",
        "    img_recon = reconstruct_img(vae, img_r, digit_r, is_hw_r)    #B\n",
        "    compare_images(img_r, img_recon)    #B\n",
        "    print(\"Generate a random image from the model:\")    #C\n",
        "    img_gen, digit_gen, is_hw_gen = generate_data(vae)    #C\n",
        "    plot_image(img_gen, \"Generated Image\")    #C\n",
        "    print(\"Intended digit: \", int(digit_gen))    #C\n",
        "    print(\"Intended as handwritten: \", bool(is_hw_gen == 1))    #C\n",
        "#A Calculate and print test loss.\n",
        "#B Compare a random test image to its reconstruction.\n",
        "#C Generate a random image from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmvAUfzZ3X4M"
      },
      "source": [
        "## Listing 5.15: Running training and plotting progress\n",
        "\n",
        "Finally, we run training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "krc0vcPv3Zqx",
        "outputId": "27747b85-41e6-4a95-abbe-287b9fcfdf59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n",
            "torch.Size([256, 50]) torch.Size([256, 50])\n",
            "torch.Size([256, 50])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, NUM_EPOCHS+\u001b[32m1\u001b[39m):    \u001b[38;5;66;03m#A\u001b[39;00m\n\u001b[32m      8\u001b[39m     loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_handwritten\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mUSE_CUDA\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1434\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1436\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 2500\n",
        "TEST_FREQUENCY = 10\n",
        "\n",
        "train_loss = []\n",
        "train_size = len(train_loader.dataset)\n",
        "\n",
        "for epoch in range(0, NUM_EPOCHS+1):    #A\n",
        "    loss = 0\n",
        "    for img, digit, is_handwritten in train_loader:\n",
        "        batch_size = img.shape[0]\n",
        "        if USE_CUDA:\n",
        "            img = img.cuda()\n",
        "            digit = digit.cuda()\n",
        "            is_handwritten = is_handwritten.cuda()\n",
        "        img, digit, is_handwritten = reshape_data(\n",
        "            img, digit, is_handwritten\n",
        "        )\n",
        "        loss += svi.step(    #B\n",
        "            img, digit, is_handwritten, batch_size    #B\n",
        "        )    #B\n",
        "    avg_loss = loss / train_size\n",
        "    print(\"Epoch: {} avgs training loss: {}\".format(epoch, loss))\n",
        "    train_loss.append(avg_loss)\n",
        "    if epoch % TEST_FREQUENCY == 0:    #C\n",
        "        test_epoch(vae, test_loader)    #C\n",
        "#A Run the training procedure for a certain number of epochs.\n",
        "#B Run a training step on one batch in one epoch.\n",
        "#C The test data evaluation procedure runs every 10 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3m6t3b8G4Fg"
      },
      "source": [
        "We can continue to use `generate_data` to generate from the model once we've trained it. Finally, we can save the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxgJvMrecHJ5"
      },
      "outputs": [],
      "source": [
        "#torch.save(vae.state_dict(), 'mnist_tmnist_weights.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
