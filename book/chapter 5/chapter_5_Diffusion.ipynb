{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ispDL1G4Fc"
      },
      "source": [
        "# Chapter 5 - Connecting Causality and Deep Learning\n",
        "\n",
        "The notebook is a code companion to chapter 5 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/altdeep/causalML/blob/master/book/chapter%205/chapter_5_Connecting_Causality_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook was written in Google Colab using Python version 3.10.12. The versions of the main libraries include:\n",
        "* pyro version 1.84\n",
        "* torch version 2.2.1\n",
        "* pandas version 2.0.3\n",
        "* torchvision vserions 0.18.0+cu121\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVk-jq9hxOYo"
      },
      "source": [
        "Pgmpy allows us to fit conventional Bayesian networks on a causal DAG. However, with modern deep probabilistic machine learning frameworks like pyro, we can build more nuanced and powerful causal models.  In this tutorial, we fit a variational autoencoder on a causal DAG that represents a dataset that mixes handwritten MNIST digits and typed T-MNIST images.\n",
        "\n",
        "![TMNIST-MNIST](https://github.com/altdeep/causalML/blob/master/book/chapter%205/images/MNIST-TMNIST.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_gEnU8rWN3e",
        "outputId": "ddeb6bd9-07ae-4db7-ca56-9c1d2d6a5603"
      },
      "outputs": [],
      "source": [
        "# !pip install pyro-ppl==1.8.4\n",
        "# !pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsu5QxOW0aAx"
      },
      "source": [
        "## Listing 5.1: Setup for GPU training\n",
        "\n",
        "The code will run faster if we use CUDA, if it's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "lfWTRtPoWF5H"
      },
      "outputs": [],
      "source": [
        "import torch    #A\n",
        "USE_CUDA = True    #A\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    #A\n",
        "#A Use CUDA if it is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UoHQdJpG4Fe"
      },
      "source": [
        "## Listing 5.2: Combining the data\n",
        "\n",
        "First, we create a Dataset object that will combine our two datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "_GoP_qMKWF5I"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "\n",
        "class CombinedDataset(Dataset):    #A\n",
        "    def __init__(self, csv_file):\n",
        "        self.dataset = pd.read_csv(csv_file)\n",
        "        print(self.dataset.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images = self.dataset.iloc[idx, 3:]    #B\n",
        "        images = np.array(images, dtype='float32')/255.  #B\n",
        "        images = images.reshape(28, 28)    #B\n",
        "        transform = transforms.ToTensor()    #B\n",
        "        images = transform(images)    #B\n",
        "        digits = self.dataset.iloc[idx, 2]    #C\n",
        "        digits = np.array([digits], dtype='int')    #C\n",
        "        is_handwritten = self.dataset.iloc[idx, 1]    #D\n",
        "        is_handwritten = np.array([is_handwritten], dtype='float32')    #D\n",
        "        return images, digits, is_handwritten    #E\n",
        "\n",
        "#A This class loads and processes a dataset that combines the MNIST and Typeface MNIST. The output is a torch.utils.data.Dataset object.\n",
        "#B Load, normalize, and reshape the images to a 28x28 pixel.\n",
        "#C Get and process the digits labels, 0-9.\n",
        "#D 1 for handwritten digits (MNIST) 0 for “typed’ digits (TMNIST).\n",
        "#E Return tuple of the image, the digit label, and the is_handwritten label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UGXmWzuJlpL"
      },
      "source": [
        "## Listing 5.3: Downloading, splitting and loading the data\n",
        "\n",
        "Next, we'll download the data and create the combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "S4jn_XvTJlxH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):    #A\n",
        "    combined_dataset = CombinedDataset(\n",
        "\"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
        "    )\n",
        "    n = len(combined_dataset)    #B\n",
        "    train_size = int(0.8 * n)    #B\n",
        "    test_size = n - train_size    #B\n",
        "    train_dataset, test_dataset = random_split(    #B\n",
        "        combined_dataset,    #B\n",
        "        [train_size, test_size],    #B\n",
        "        generator=torch.Generator().manual_seed(42)    #B\n",
        "    )    #B\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    train_loader = DataLoader(    #C\n",
        "        train_dataset,    #C\n",
        "        batch_size=batch_size,    #C\n",
        "        shuffle=True,    #C\n",
        "        **kwargs    #C\n",
        "    )    #C\n",
        "    test_loader = DataLoader(    #C\n",
        "        test_dataset,    #C\n",
        "        batch_size=batch_size,    #C\n",
        "        shuffle=True,    #C\n",
        "        **kwargs    #C\n",
        "    )    #C\n",
        "    return train_loader, test_loader\n",
        "#A Setup data loader that loads the data and splits it into training and test sets\n",
        "#B Allot 80% of the data to training data, the remaining 20% to test data.\n",
        "#C Create training and test loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 787)\n"
          ]
        }
      ],
      "source": [
        "train_loader, test_loader = setup_dataloaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "column 0 is simply index of the datapoint in the dataset, column 1 is is_handwritten, column 2 is digits, columns 3: are image values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRXUclDPpKpd"
      },
      "source": [
        "## Listing 5.4: Implement the decoder\n",
        "\n",
        "First, we specify a decoder. The decoder maps the latent variable Z, a variable representing the value of the digit, and a binary variable representing whether the digit is handwritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# This is the diffusion reverse model that predicts the conditional mean of the noise epsilon_t at any time step t given the noisy image x_t and the causal conditioning variables.\n",
        "# The output is a vector of size img_dim with real values that directly denote the predicted noise that was added during the forward process - not a distribution.\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, T):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28\n",
        "        digit_dim = 10\n",
        "        is_handwritten_dim = 1\n",
        "        t_dim = 1\n",
        "        self.T = T\n",
        "\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "        encoding_dim = img_dim + digit_dim + is_handwritten_dim + t_dim\n",
        "        self.fc1 = nn.Linear(encoding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, img_dim)\n",
        "\n",
        "    def forward(self, x_t, digit, is_handwritten, t):\n",
        "        x_t = x_t.view(x_t.size(0), -1)\n",
        "        t_norm = (t.unsqueeze(1).float() / self.T)\n",
        "        input = torch.cat([x_t, digit, is_handwritten, t_norm], dim=1)\n",
        "        h1 = self.softplus(self.fc1(input))\n",
        "        h2 = self.softplus(self.fc2(h1))\n",
        "        eps_hat = self.fc3(h2)\n",
        "        return eps_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, T=1000, hidden_dim=400, use_cuda=USE_CUDA):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.T = T\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # 1. Setup the reverse-process network\n",
        "        self.decoder = Decoder(hidden_dim, T=T)\n",
        "        \n",
        "        # 2. Setup fixed diffusion schedule\n",
        "        self.setup_schedule()\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def setup_schedule(self):\n",
        "        \"\"\"Precompute alpha-bars. Using register_buffer ensures these \n",
        "        move to the GPU automatically with the model.\"\"\"\n",
        "        betas = torch.linspace(1e-4, 0.02, self.T)\n",
        "        alphas = 1.0 - betas\n",
        "        # Registering as buffer handles device placement during .cuda() calls\n",
        "        self.register_buffer('alpha_bars', torch.cumprod(alphas, dim=0))\n",
        "        self.register_buffer('betas', betas)\n",
        "        self.register_buffer('alphas', alphas)\n",
        "\n",
        "    def q_sample(self, x0, t, eps):\n",
        "        \"\"\"The forward process: x0 -> xt. Anchor all constants to x0.device.\"\"\"\n",
        "        # Ensure t is an index tensor on the same device as alpha_bars\n",
        "        t = t.to(self.alpha_bars.device)\n",
        "        \n",
        "        # Extract coefficients and ensure they are on the correct device\n",
        "        sqrt_ab = self.alpha_bars[t].sqrt().unsqueeze(1).to(x0.device)\n",
        "        sqrt_1_ab = (1 - self.alpha_bars[t]).sqrt().unsqueeze(1).to(x0.device)\n",
        "        \n",
        "        return sqrt_ab * x0 + sqrt_1_ab * eps\n",
        "\n",
        "    def model(self, x0, data_size=1):\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        device = x0.device # This is our 'anchor' device\n",
        "        options = dict(dtype=torch.float32, device=device)\n",
        "        \n",
        "        # 1. Sample diffusion time (Batched)\n",
        "        # Probabilities must be on the same device as the data\n",
        "        batched_probs = torch.ones(data_size, self.T, **options) / self.T\n",
        "        t = pyro.sample(\"t\", dist.Categorical(batched_probs))\n",
        "\n",
        "        # 2. Sample exogenous noise (The latent 'style' equivalent)\n",
        "        # Using zeros_like(x0) guarantees eps is on x0's device\n",
        "        eps = pyro.sample(\"eps\", dist.Normal(torch.zeros_like(x0), 1.0).to_event(1))\n",
        "        \n",
        "        # 3. Sample Causal Parents\n",
        "        p_digit = torch.ones(data_size, 10, **options) / 10\n",
        "        digit = pyro.sample(\"digit\", dist.OneHotCategorical(p_digit))\n",
        "\n",
        "        p_is_hw = torch.ones(data_size, 1, **options) / 2\n",
        "        is_handwritten = pyro.sample(\"is_handwritten\", dist.Bernoulli(p_is_hw).to_event(1))\n",
        "\n",
        "        # 4. Forward diffusion\n",
        "        x_t = self.q_sample(x0, t, eps)\n",
        "\n",
        "        # 5. Predict noise (The Decoder handles its own device internally via pyro.module)\n",
        "        eps_hat = self.decoder(x_t, digit, is_handwritten, t)\n",
        "\n",
        "        # 6. Observation\n",
        "        pyro.sample(\"obs_eps\", dist.Normal(eps_hat, 0.1).to_event(1), obs=eps)\n",
        "\n",
        "        return x_t, eps, digit, is_handwritten\n",
        "\n",
        "    def training_model(self, img, digit, is_handwritten, batch_size):\n",
        "        # Conditioning inherits the device of the tensors passed in\n",
        "        conditioned_on_data = pyro.condition(\n",
        "            self.model,\n",
        "            data={\"digit\": digit, \"is_handwritten\": is_handwritten}\n",
        "        )\n",
        "        with pyro.plate(\"data\", batch_size):\n",
        "            return conditioned_on_data(x0=img, data_size=batch_size)\n",
        "\n",
        "    def training_guide(self, img, digit, is_handwritten, batch_size):\n",
        "        \"\"\"The guide must mirror the model's device placement.\"\"\"\n",
        "        device = img.device\n",
        "        with pyro.plate(\"data\", batch_size):\n",
        "            # Anchor probabilities to the image device\n",
        "            batched_probs = torch.ones(batch_size, self.T, device=device) / self.T\n",
        "            pyro.sample(\"t\", dist.Categorical(batched_probs))\n",
        "            \n",
        "            # Anchor noise to the image device\n",
        "            pyro.sample(\"eps\", dist.Normal(torch.zeros_like(img), 1.0).to_event(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8_IhBKK0kpO"
      },
      "source": [
        "## Listing 5.10 Helper function for plotting images\n",
        "\n",
        "The following utility functions helps us visualize progress during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "ohxiEjB30lHa"
      },
      "outputs": [],
      "source": [
        "def plot_image(img, title=None):    #A\n",
        "    fig = plt.figure()\n",
        "    img_to_show = (img.cpu().detach() + 1.0) / 2.0\n",
        "    plt.imshow(img_to_show, cmap='Greys_r', interpolation='nearest')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "#A Helper function for plotting an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTFLSyVW06tr"
      },
      "source": [
        "## Listing 5.11: Define a helper functions for reconstructing and viewing the images\n",
        "\n",
        "These additional utility functions help us selected and reshape images, as well as generate new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def reconstruct_img(diffusion, img, digit, is_hw, t_val=500, use_cuda=USE_CUDA):\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    # 1. Prepare Inputs - Ensure all are (1, Features)\n",
        "    img = img.reshape(1, -1).to(device) \n",
        "    \n",
        "    # Create digit tensor correctly\n",
        "    digit_tensor = torch.zeros(1, 10, device=device)\n",
        "    digit_tensor[0, digit] = 1.0\n",
        "    \n",
        "    # Create is_hw tensor as (1, 1)\n",
        "    is_hw_tensor = torch.tensor([[float(is_hw)]], device=device)\n",
        "    \n",
        "    # Create t_tensor as (1,)\n",
        "    t_tensor = torch.tensor([t_val], device=device)\n",
        "\n",
        "    # 2. Add Noise\n",
        "    eps = torch.randn_like(img)\n",
        "    x_t = diffusion.q_sample(img, t_tensor, eps)\n",
        "\n",
        "    # 3. Predict the Noise\n",
        "    eps_hat = diffusion.decoder(x_t, digit_tensor, is_hw_tensor, t_tensor)\n",
        "\n",
        "    # 4. Mathematical Reconstruction\n",
        "    # Anchor alpha_bars to the correct device\n",
        "    alpha_bar_t = diffusion.alpha_bars[t_tensor].to(device)\n",
        "    sqrt_alpha_bar = alpha_bar_t.sqrt().unsqueeze(1)\n",
        "    sqrt_one_minus_alpha_bar = (1 - alpha_bar_t).sqrt().unsqueeze(1)\n",
        "    \n",
        "    img_reconstructed = (x_t - sqrt_one_minus_alpha_bar * eps_hat) / sqrt_alpha_bar\n",
        "\n",
        "    return img_reconstructed.view(28, 28).detach()\n",
        "\n",
        "def compare_images(original, reconstruction):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    images = [original, reconstruction]\n",
        "    titles = ['Original', 'Reconstructed']\n",
        "    \n",
        "    for ax, img, title in zip(axes, images, titles):\n",
        "        ax.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79MHVV7313mT"
      },
      "source": [
        "## Listing 5.12: Data processing helper functions for training\n",
        "\n",
        "Next, we'll create some helper functions for handling the data. We'll use `get_random_example` to grab random images from the dataset. `reshape_data` will convert an image and its labels into input for the encoder. We'll use `generate_data` and `generate_coded_data` will simulate an image from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_random_example(loader):    #A\n",
        "    random_idx = np.random.randint(0, len(loader.dataset))    #A\n",
        "    img, digit, is_handwritten = loader.dataset[random_idx]    #A\n",
        "    return img.squeeze(), digit, is_handwritten    #A\n",
        "\n",
        "def reshape_data(img, digit, is_handwritten):    #B\n",
        "    digit = F.one_hot(digit, 10).squeeze()    #B\n",
        "    img = (img.reshape(-1, 28*28) * 2.0) - 1.0    #B\n",
        "    return img, digit, is_handwritten    #B\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(diffusion, x_t, t, digit, is_hw):\n",
        "    \"\"\"\n",
        "    Reverse diffusion step: samples x_{t-1} given x_t.\n",
        "    \"\"\"\n",
        "    # 1. Predict the noise using the decoder\n",
        "    eps_hat = diffusion.decoder(x_t, digit, is_hw, t)\n",
        "\n",
        "    # 2. Calculate coefficients for the reverse step\n",
        "    # Based on the DDPM math: x_{t-1} = 1/sqrt(alpha) * (x_t - beta/sqrt(1-ab) * eps_hat)\n",
        "    beta_t = diffusion.betas[t]\n",
        "    sqrt_one_minus_alpha_bar_t = (1 - diffusion.alpha_bars[t]).sqrt()\n",
        "    sqrt_alpha_t = diffusion.alphas[t].sqrt()\n",
        "\n",
        "    # The predicted mean of x_{t-1}\n",
        "    mean = (1 / sqrt_alpha_t) * (x_t - (beta_t / sqrt_one_minus_alpha_bar_t) * eps_hat)\n",
        "\n",
        "    if t == 0:\n",
        "        return mean\n",
        "    else:\n",
        "        # Add a bit of 'fresh' noise (Langevin dynamics) to keep generation diverse\n",
        "        noise = torch.randn_like(x_t)\n",
        "        sigma_t = beta_t.sqrt() # Standard DDPM variance choice\n",
        "        return mean + sigma_t * noise\n",
        "    \n",
        "def generate_coded_data(diffusion, use_cuda=USE_CUDA):\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    # 1. Start with Pure Noise (The Diffusion 'Latent')\n",
        "    img_shape = (1, 28 * 28)\n",
        "    x = torch.randn(img_shape, device=device)\n",
        "\n",
        "    # 2. Sample Causal Parents (Digit and Style)\n",
        "    p_digit = torch.ones(1, 10, device=device) / 10\n",
        "    digit = dist.OneHotCategorical(p_digit).sample()\n",
        "    \n",
        "    p_is_hw = torch.ones(1, 1, device=device) / 2\n",
        "    is_handwritten = dist.Bernoulli(p_is_hw).sample()\n",
        "\n",
        "    # 3. The Reverse Loop (The 'Generation' work)\n",
        "    # We step from t=999 all the way down to t=0\n",
        "    for t_idx in reversed(range(diffusion.T)):\n",
        "        t_tensor = torch.tensor([t_idx], device=device)\n",
        "        x = p_sample(diffusion, x, t_tensor, digit, is_handwritten)\n",
        "\n",
        "    return x, digit, is_handwritten\n",
        "\n",
        "def generate_data(diffusion, use_cuda=USE_CUDA):\n",
        "    # This remains the wrapper that cleans up shapes for plotting\n",
        "    img, digit, is_handwritten = generate_coded_data(diffusion, use_cuda)\n",
        "    \n",
        "    img = img.squeeze().view(28, 28).detach()\n",
        "    digit = torch.argmax(digit, 1)\n",
        "    # is_handwritten for Bernoulli is usually just the value\n",
        "    is_handwritten = is_handwritten.squeeze().round().int() \n",
        "    \n",
        "    return img, digit, is_handwritten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFp3sIBu2-eW"
      },
      "source": [
        "## Listing 5.13: Set up the training procedure\n",
        "\n",
        "Next we set up traing. The training objective `Trace_ELBO` simultaneously trains the parameters of the encoder and the decoder. It focuses on minimizing reconstruction error (how much information is lost when an image encoded, and then decoded once again) and [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the distribution modeled by the guide (the variational distribution) and the P(Z|image, is_handwritten, digit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 787)\n"
          ]
        }
      ],
      "source": [
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# 1. Clear previous parameters\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# 2. Initialize the Diffusion Class\n",
        "T_steps = 2000\n",
        "diffusion_model = Diffusion(T=T_steps, hidden_dim=400)\n",
        "train_loader, test_loader = setup_dataloaders(batch_size=256)\n",
        "\n",
        "# 3. Setup Optimizer\n",
        "# We use a slightly lower learning rate often preferred for Diffusion\n",
        "optimizer = Adam({\"lr\": 1.0e-3})\n",
        "\n",
        "# 4. Define Model and Guide\n",
        "# We point SVI to the 'training' versions we wrote\n",
        "model = diffusion_model.training_model\n",
        "guide = diffusion_model.training_guide\n",
        "\n",
        "# 5. Initialize SVI\n",
        "# Trace_ELBO works here because our 'model' has an 'obs=' statement \n",
        "# and our 'guide' samples the same latent variables (t, eps).\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klKFXtLpTOos"
      },
      "source": [
        "## Listing 5.14: Setting up a test evaluation procedure\n",
        "\n",
        "When training generative models, it is useful to setup a procedure that uses test data to evaluate how well training is progressing. You can include anything you think is useful to monitor during training. Here, I calculate and print the loss function on the test data, just to make sure test loss is progressively decreasing along with training loss (a flattening of test loss while training loss continued to decrease would indicate overfitting).\n",
        "\n",
        "But a more direct signal at how well our model is training is to generate and view images. In my test evaluation procedure, I produce two visualizations. First, I inspect how well it can reconstruct a random image from the test data. I pass the image through the encoder then through the decoder, creating a “reconstruction” of the image. Then I plot the original and reconstructed image side-by-side and compare them visually, looking to see that they are close to identical.\n",
        "\n",
        "Next, I visualize how well it is performing as an overall generative model by generating and plotting an image from scratch. I run this code once each time a certain number of epochs are run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_epoch(diffusion, test_loader, epoch):\n",
        "    epoch_loss_test = 0\n",
        "    \n",
        "    # 1. Calculate Test Loss (Quantitative Check)\n",
        "    for img, digit, is_hw in test_loader:\n",
        "        batch_size = img.shape[0]\n",
        "        if USE_CUDA:\n",
        "            img, digit, is_hw = img.cuda(), digit.cuda(), is_hw.cuda()\n",
        "            \n",
        "        img, digit, is_hw = reshape_data(img, digit, is_hw)\n",
        "        \n",
        "        # evaluate_loss does everything step() does but WITHOUT updating weights\n",
        "        epoch_loss_test += svi.evaluate_loss(img, digit, is_hw, batch_size)\n",
        "        \n",
        "    test_size = len(test_loader.dataset)\n",
        "    avg_loss = epoch_loss_test / test_size\n",
        "    print(f\"Epoch: {epoch} avg. test loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 2. Visualize Reconstruction (Qualitative Check: Denoising)\n",
        "    print(\"Comparing a random test image to its (one-step) reconstruction:\")\n",
        "    img_r, digit_r, is_hw_r = get_random_example(test_loader)\n",
        "    \n",
        "    # We use a mid-range t (e.g., 400) to see if it can recover from significant noise\n",
        "    t_test = 1000 \n",
        "    img_recon = reconstruct_img(diffusion, img_r, digit_r, is_hw_r, t_val=t_test)\n",
        "    \n",
        "    # You might want to modify compare_images to show the original vs reconstruction\n",
        "    compare_images(img_r.view(28, 28), img_recon)\n",
        "\n",
        "    # 3. Generate New Image (Qualitative Check: Full Reverse Process)\n",
        "    print(\"Generate a brand new image by sampling from pure noise:\")\n",
        "    # This calls your generate_data function with the 1000-step loop\n",
        "    img_gen, digit_gen, is_hw_gen = generate_data(diffusion)\n",
        "    \n",
        "    plot_image(img_gen, f\"Generated: Digit {int(digit_gen)}\")\n",
        "    print(f\"Intended digit: {int(digit_gen)}\")\n",
        "    print(f\"Intended as handwritten: {bool(is_hw_gen == 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmvAUfzZ3X4M"
      },
      "source": [
        "## Listing 5.15: Running training and plotting progress\n",
        "\n",
        "Finally, we run training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 | Avg Training Loss (ELBO): 36720.1081\n",
            "Epoch: 0 avg. test loss: 33039.8049\n",
            "Comparing a random test image to its (one-step) reconstruction:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/scratch/a/jshreeku/tmp/ipykernel_3012038/2670287072.py:14: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  is_hw_tensor = torch.tensor([[float(is_hw)]], device=device)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generate a brand new image by sampling from pure noise:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intended digit: 2\n",
            "Intended as handwritten: False\n",
            "Epoch: 1 | Avg Training Loss (ELBO): 29665.8902\n",
            "Epoch: 2 | Avg Training Loss (ELBO): 26527.1746\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, NUM_EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m      8\u001b[39m     loss = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_handwritten\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mUSE_CUDA\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1434\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1436\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 2500\n",
        "TEST_FREQUENCY = 100  # Note: Generation is slow, so you might increase this later\n",
        "\n",
        "train_loss = []\n",
        "train_size = len(train_loader.dataset)\n",
        "\n",
        "for epoch in range(0, NUM_EPOCHS + 1):\n",
        "    loss = 0\n",
        "    for img, digit, is_handwritten in train_loader:\n",
        "        batch_size = img.shape[0]\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            img = img.cuda()\n",
        "            digit = digit.cuda()\n",
        "            is_handwritten = is_handwritten.cuda()\n",
        "            \n",
        "        # Re-use your existing reshape_data function\n",
        "        img, digit, is_handwritten = reshape_data(img, digit, is_handwritten)\n",
        "        \n",
        "        # SVI.step logic:\n",
        "        # This calls: training_model(img, digit, is_handwritten, batch_size)\n",
        "        # And: training_guide(img, digit, is_handwritten, batch_size)\n",
        "        loss += svi.step(img, digit, is_handwritten, batch_size)\n",
        "        \n",
        "    avg_loss = loss / train_size\n",
        "    train_loss.append(avg_loss)\n",
        "    \n",
        "    print(f\"Epoch: {epoch} | Avg Training Loss (ELBO): {avg_loss:.4f}\")\n",
        "\n",
        "    # Run the qualitative and quantitative test procedure\n",
        "    if epoch % TEST_FREQUENCY == 0:\n",
        "        # Note: We pass 'diffusion_model' (your class instance) instead of 'vae'\n",
        "        test_epoch(diffusion_model, test_loader, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3m6t3b8G4Fg"
      },
      "source": [
        "We can continue to use `generate_data` to generate from the model once we've trained it. Finally, we can save the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxgJvMrecHJ5"
      },
      "outputs": [],
      "source": [
        "#torch.save(vae.state_dict(), 'mnist_tmnist_weights.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pyro.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784 + 10 + 1 + 1, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 784)\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, x_t, digit, is_hw, t):\n",
        "        x_t = x_t.view(x_t.size(0), -1)\n",
        "\n",
        "        digit = digit.view(digit.size(0), -1)   # 🔧 force 2D\n",
        "        is_hw = is_hw.view(is_hw.size(0), -1)   # 🔧 force 2D\n",
        "        t = t.view(t.size(0), 1).float()         # 🔧 force 2D\n",
        "\n",
        "        h = torch.cat([x_t, digit, is_hw, t], dim=1)\n",
        "        h = self.softplus(self.fc1(h))\n",
        "        return self.fc2(h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, T=1000, hidden_dim=400):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.decoder = Decoder(hidden_dim)\n",
        "        self.setup_schedule()\n",
        "\n",
        "    def setup_schedule(self):\n",
        "        betas = torch.linspace(1e-4, 0.02, self.T)\n",
        "        alphas = 1.0 - betas\n",
        "        self.register_buffer(\"betas\", betas)\n",
        "        self.register_buffer(\"alphas\", alphas)\n",
        "        self.register_buffer(\"alpha_bars\", torch.cumprod(alphas, dim=0))\n",
        "\n",
        "    def q_sample(self, x0, t, eps):\n",
        "        sqrt_ab = self.alpha_bars[t].sqrt().unsqueeze(1)\n",
        "        sqrt_1_ab = (1 - self.alpha_bars[t]).sqrt().unsqueeze(1)\n",
        "        return sqrt_ab * x0 + sqrt_1_ab * eps\n",
        "\n",
        "    # ✅ THIS MUST BE INSIDE THE CLASS\n",
        "    def model(self, x0, digit, is_hw):\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "\n",
        "        batch_size = x0.size(0)\n",
        "        device = x0.device\n",
        "\n",
        "        t = pyro.sample(\n",
        "            \"t\",\n",
        "            dist.Categorical(torch.ones(batch_size, self.T, device=device) / self.T)\n",
        "        )\n",
        "\n",
        "        eps = pyro.sample(\n",
        "            \"eps\",\n",
        "            dist.Normal(torch.zeros_like(x0), 1.0).to_event(1)\n",
        "        )\n",
        "\n",
        "        x_t = self.q_sample(x0, t, eps)\n",
        "        eps_hat = self.decoder(x_t, digit, is_hw, t)\n",
        "\n",
        "        mse = ((eps - eps_hat) ** 2).sum(dim=1)\n",
        "        pyro.factor(\"diffusion_loss\", -mse.mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(diffusion, x_t, t, digit, is_hw):\n",
        "    eps_hat = diffusion.decoder(x_t, digit, is_hw, t)\n",
        "\n",
        "    beta = diffusion.betas[t]\n",
        "    alpha = diffusion.alphas[t]\n",
        "    alpha_bar = diffusion.alpha_bars[t]\n",
        "\n",
        "    mean = (1 / alpha.sqrt()) * (\n",
        "        x_t - (beta / (1 - alpha_bar).sqrt()) * eps_hat\n",
        "    )\n",
        "\n",
        "    if t.item() == 0:\n",
        "        return mean\n",
        "\n",
        "    noise = torch.randn_like(x_t)\n",
        "    return mean + beta.sqrt() * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def generate(diffusion):\n",
        "    device = next(diffusion.parameters()).device\n",
        "    x = torch.randn(1, 28*28, device=device)\n",
        "\n",
        "    digit = F.one_hot(torch.randint(0, 10, (1,)), 10).float().to(device)\n",
        "    is_hw = torch.bernoulli(torch.ones(1,1, device=device) * 0.5)\n",
        "\n",
        "    for t in reversed(range(diffusion.T)):\n",
        "        x = p_sample(diffusion, x, torch.tensor([t], device=device), digit, is_hw)\n",
        "\n",
        "    return x.view(28,28), digit.argmax(), is_hw.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyro import poutine\n",
        "\n",
        "def train(diffusion, train_loader, epochs=1000, lr=1e-3):\n",
        "    optimizer = torch.optim.Adam(diffusion.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for img, digit, is_hw in train_loader:\n",
        "            img = img.view(img.size(0), -1)\n",
        "            digit = F.one_hot(digit.view(-1), 10).float()\n",
        "            is_hw = is_hw.view(-1, 1).float()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 1️⃣ Sample t and eps for diffusion\n",
        "            batch_size = img.size(0)\n",
        "            t = torch.randint(0, diffusion.T, (batch_size,), device=img.device)\n",
        "            eps = torch.randn_like(img)\n",
        "\n",
        "            # 2️⃣ Forward diffusion\n",
        "            x_t = diffusion.q_sample(img, t, eps)\n",
        "\n",
        "            # 3️⃣ Predict noise\n",
        "            eps_hat = diffusion.decoder(x_t, digit, is_hw, t)\n",
        "\n",
        "            # 4️⃣ MSE loss\n",
        "            loss = ((eps - eps_hat) ** 2).mean()\n",
        "\n",
        "            # 5️⃣ Backprop\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch} | Avg Loss: {total_loss / len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 787)\n",
            "Epoch 0 | Avg Loss: 2.2540\n",
            "Epoch 1 | Avg Loss: 0.8087\n",
            "Epoch 2 | Avg Loss: 0.7017\n",
            "Epoch 3 | Avg Loss: 0.6591\n",
            "Epoch 4 | Avg Loss: 0.6382\n",
            "Epoch 5 | Avg Loss: 0.6222\n",
            "Epoch 6 | Avg Loss: 0.6133\n",
            "Epoch 7 | Avg Loss: 0.6103\n",
            "Epoch 8 | Avg Loss: 0.6046\n",
            "Epoch 9 | Avg Loss: 0.5988\n",
            "Epoch 10 | Avg Loss: 0.5931\n",
            "Epoch 11 | Avg Loss: 0.5860\n",
            "Epoch 12 | Avg Loss: 0.5773\n",
            "Epoch 13 | Avg Loss: 0.5731\n",
            "Epoch 14 | Avg Loss: 0.5676\n",
            "Epoch 15 | Avg Loss: 0.5623\n",
            "Epoch 16 | Avg Loss: 0.5580\n",
            "Epoch 17 | Avg Loss: 0.5567\n",
            "Epoch 18 | Avg Loss: 0.5546\n",
            "Epoch 19 | Avg Loss: 0.5524\n",
            "Epoch 20 | Avg Loss: 0.5508\n",
            "Epoch 21 | Avg Loss: 0.5506\n",
            "Epoch 22 | Avg Loss: 0.5488\n",
            "Epoch 23 | Avg Loss: 0.5498\n",
            "Epoch 24 | Avg Loss: 0.5493\n",
            "Epoch 25 | Avg Loss: 0.5495\n",
            "Epoch 26 | Avg Loss: 0.5492\n",
            "Epoch 27 | Avg Loss: 0.5487\n",
            "Epoch 28 | Avg Loss: 0.5501\n",
            "Epoch 29 | Avg Loss: 0.5498\n",
            "Epoch 30 | Avg Loss: 0.5485\n",
            "Epoch 31 | Avg Loss: 0.5491\n",
            "Epoch 32 | Avg Loss: 0.5491\n",
            "Epoch 33 | Avg Loss: 0.5481\n",
            "Epoch 34 | Avg Loss: 0.5501\n",
            "Epoch 35 | Avg Loss: 0.5490\n",
            "Epoch 36 | Avg Loss: 0.5499\n",
            "Epoch 37 | Avg Loss: 0.5498\n",
            "Epoch 38 | Avg Loss: 0.5493\n",
            "Epoch 39 | Avg Loss: 0.5492\n",
            "Epoch 40 | Avg Loss: 0.5488\n",
            "Epoch 41 | Avg Loss: 0.5487\n",
            "Epoch 42 | Avg Loss: 0.5493\n",
            "Epoch 43 | Avg Loss: 0.5500\n",
            "Epoch 44 | Avg Loss: 0.5495\n",
            "Epoch 45 | Avg Loss: 0.5488\n",
            "Epoch 46 | Avg Loss: 0.5487\n",
            "Epoch 47 | Avg Loss: 0.5491\n",
            "Epoch 48 | Avg Loss: 0.5481\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m T_steps = \u001b[32m2000\u001b[39m\n\u001b[32m      9\u001b[39m diffusion_model = Diffusion(T=T_steps, hidden_dim=\u001b[32m400\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(diffusion, train_loader, epochs, lr)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m      7\u001b[39m     total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_hw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdigit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1434\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1432\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1434\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1435\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1436\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1263\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1264\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1274\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1278\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1279\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1280\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "\n",
        "train_loader, test_loader = setup_dataloaders(batch_size=256)\n",
        "\n",
        "# 3. Setup Optimizer\n",
        "# We use a slightly lower learning rate often preferred for Diffusion\n",
        "optimizer = Adam({\"lr\": 1.0e-3})\n",
        "\n",
        "\n",
        "T_steps = 2000\n",
        "diffusion_model = Diffusion(T=T_steps, hidden_dim=400)\n",
        "\n",
        "\n",
        "train(diffusion=diffusion_model, train_loader=train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['_INPUT', 'decoder$$$fc1.weight', 'decoder$$$fc1.bias', 'decoder$$$fc2.weight', 'decoder$$$fc2.bias', 't', 'eps', 'diffusion_loss', '_RETURN'])\n",
            "torch.Size([0])\n"
          ]
        }
      ],
      "source": [
        "img, digit, is_hw = next(iter(train_loader))\n",
        "img = img.view(img.size(0), -1)\n",
        "digit = F.one_hot(digit.view(-1), 10).float()\n",
        "is_hw = is_hw.view(-1, 1).float()\n",
        "\n",
        "trace = poutine.trace(diffusion_model.model).get_trace(img, digit, is_hw)\n",
        "print(trace.nodes.keys())\n",
        "print(trace.nodes[\"diffusion_loss\"][\"value\"].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
