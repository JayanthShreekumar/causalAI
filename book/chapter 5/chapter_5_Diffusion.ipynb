{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1ispDL1G4Fc"
      },
      "source": [
        "# Chapter 5 - Connecting Causality and Deep Learning\n",
        "\n",
        "The notebook is a code companion to chapter 5 of the book [Causal AI](https://www.manning.com/books/causal-ai) by [Robert Osazuwa Ness](https://www.linkedin.com/in/osazuwa/).\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/altdeep/causalML/blob/master/book/chapter%205/chapter_5_Connecting_Causality_and_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "This notebook was written in Google Colab using Python version 3.10.12. The versions of the main libraries include:\n",
        "* pyro version 1.84\n",
        "* torch version 2.2.1\n",
        "* pandas version 2.0.3\n",
        "* torchvision vserions 0.18.0+cu121\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVk-jq9hxOYo"
      },
      "source": [
        "Pgmpy allows us to fit conventional Bayesian networks on a causal DAG. However, with modern deep probabilistic machine learning frameworks like pyro, we can build more nuanced and powerful causal models.  In this tutorial, we fit a variational autoencoder on a causal DAG that represents a dataset that mixes handwritten MNIST digits and typed T-MNIST images.\n",
        "\n",
        "![TMNIST-MNIST](https://github.com/altdeep/causalML/blob/master/book/chapter%205/images/MNIST-TMNIST.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_gEnU8rWN3e",
        "outputId": "ddeb6bd9-07ae-4db7-ca56-9c1d2d6a5603"
      },
      "outputs": [],
      "source": [
        "# !pip install pyro-ppl==1.8.4\n",
        "# !pip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsu5QxOW0aAx"
      },
      "source": [
        "## Listing 5.1: Setup for GPU training\n",
        "\n",
        "The code will run faster if we use CUDA, if it's available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lfWTRtPoWF5H"
      },
      "outputs": [],
      "source": [
        "import torch    #A\n",
        "USE_CUDA = True    #A\n",
        "DEVICE_TYPE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")    #A\n",
        "#A Use CUDA if it is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UoHQdJpG4Fe"
      },
      "source": [
        "## Listing 5.2: Combining the data\n",
        "\n",
        "First, we create a Dataset object that will combine our two datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_GoP_qMKWF5I"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "\n",
        "class CombinedDataset(Dataset):    #A\n",
        "    def __init__(self, csv_file):\n",
        "        self.dataset = pd.read_csv(csv_file)\n",
        "        print(self.dataset.shape)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        images = self.dataset.iloc[idx, 3:]    #B\n",
        "        images = np.array(images, dtype='float32')/255.  #B\n",
        "        images = images.reshape(28, 28)    #B\n",
        "        transform = transforms.ToTensor()    #B\n",
        "        images = transform(images)    #B\n",
        "        digits = self.dataset.iloc[idx, 2]    #C\n",
        "        digits = np.array([digits], dtype='int')    #C\n",
        "        is_handwritten = self.dataset.iloc[idx, 1]    #D\n",
        "        is_handwritten = np.array([is_handwritten], dtype='float32')    #D\n",
        "        return images, digits, is_handwritten    #E\n",
        "\n",
        "#A This class loads and processes a dataset that combines the MNIST and Typeface MNIST. The output is a torch.utils.data.Dataset object.\n",
        "#B Load, normalize, and reshape the images to a 28x28 pixel.\n",
        "#C Get and process the digits labels, 0-9.\n",
        "#D 1 for handwritten digits (MNIST) 0 for “typed’ digits (TMNIST).\n",
        "#E Return tuple of the image, the digit label, and the is_handwritten label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UGXmWzuJlpL"
      },
      "source": [
        "## Listing 5.3: Downloading, splitting and loading the data\n",
        "\n",
        "Next, we'll download the data and create the combined dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "S4jn_XvTJlxH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "def setup_dataloaders(batch_size=64, use_cuda=USE_CUDA):    #A\n",
        "    combined_dataset = CombinedDataset(\n",
        "\"https://raw.githubusercontent.com/altdeep/causalML/master/datasets/combined_mnist_tmnist_data.csv\"\n",
        "    )\n",
        "    n = len(combined_dataset)    #B\n",
        "    train_size = int(0.8 * n)    #B\n",
        "    test_size = n - train_size    #B\n",
        "    train_dataset, test_dataset = random_split(    #B\n",
        "        combined_dataset,    #B\n",
        "        [train_size, test_size],    #B\n",
        "        generator=torch.Generator().manual_seed(42)    #B\n",
        "    )    #B\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': use_cuda}\n",
        "    train_loader = DataLoader(    #C\n",
        "        train_dataset,    #C\n",
        "        batch_size=batch_size,    #C\n",
        "        shuffle=True,    #C\n",
        "        **kwargs    #C\n",
        "    )    #C\n",
        "    test_loader = DataLoader(    #C\n",
        "        test_dataset,    #C\n",
        "        batch_size=batch_size,    #C\n",
        "        shuffle=True,    #C\n",
        "        **kwargs    #C\n",
        "    )    #C\n",
        "    return train_loader, test_loader\n",
        "#A Setup data loader that loads the data and splits it into training and test sets\n",
        "#B Allot 80% of the data to training data, the remaining 20% to test data.\n",
        "#C Create training and test loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 787)\n"
          ]
        }
      ],
      "source": [
        "train_loader, test_loader = setup_dataloaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "column 0 is simply index of the datapoint in the dataset, column 1 is is_handwritten, column 2 is digits, columns 3: are image values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRXUclDPpKpd"
      },
      "source": [
        "## Listing 5.4: Implement the decoder\n",
        "\n",
        "First, we specify a decoder. The decoder maps the latent variable Z, a variable representing the value of the digit, and a binary variable representing whether the digit is handwritten."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pQEwj1mh-TTj"
      },
      "outputs": [],
      "source": [
        "# from torch import nn\n",
        "\n",
        "# class Decoder(nn.Module):    #A\n",
        "#     def __init__(self, z_dim, hidden_dim):\n",
        "#         super().__init__()\n",
        "#         img_dim = 28 * 28    #B\n",
        "#         digit_dim = 10    #C\n",
        "#         is_handwritten_dim = 1    #D\n",
        "#         self.softplus = nn.Softplus()    #E\n",
        "#         self.sigmoid = nn.Sigmoid()    #E\n",
        "#         encoding_dim = z_dim + digit_dim + is_handwritten_dim    #F\n",
        "#         self.fc1 = nn.Linear(encoding_dim, hidden_dim)    #F\n",
        "#         self.fc2 = nn.Linear(hidden_dim, img_dim)    #G\n",
        "\n",
        "#     def forward(self, z, digit, is_handwritten):    #H\n",
        "#         input = torch.cat([z, digit, is_handwritten], dim=1)   #I\n",
        "#         hidden = self.softplus(self.fc1(input))    #J\n",
        "#         img_param = self.sigmoid(self.fc2(hidden))    #K\n",
        "#         return img_param\n",
        "# #A The decoder method of a VAE class.\n",
        "# #B Image is 28 by 28 pixels\n",
        "# #C Digit is one-hot encoded digits 0-9, i.e., a vector of length 10.\n",
        "# #D An indicator for if the digit is handwritten that has size 1\n",
        "# #E The softplus and sigmoid are nonlinear transforms (activation functions) used in mapping between layers.\n",
        "# #F fc1 is a linear function that maps Z vector, the digit, and the is_handwritten to a linear out, which is passed through a softplus activation function to create a \"hidden layer\" - a vector whose length is given by hidden_layer.\n",
        "# #G The fc2 linearly maps the hidden layer to an output passed to a sigmoid function. The resulting value is a value between 0 and 1.\n",
        "# #H Define the forward computation from the latent Z variable value to a generated X variable value.\n",
        "# #I First combine Z and the labels.\n",
        "# #J Then compute the hidden layer.\n",
        "# #K Finally, pass the hidden layer to a linear transform, then to a sigmoid transform to output a parameter vector of length 784. Each element of the vector corresponds to a Bernoulli parameter value for an image pixel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "# This is the diffusion reverse model that predicts the conditional mean of the noise epsilon_t at any time step t given the noisy image x_t and the causal conditioning variables.\n",
        "# The output is a vector of size img_dim with real values that directly denote the predicted noise that was added during the forward process - not a distribution.\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28\n",
        "        digit_dim = 10\n",
        "        is_handwritten_dim = 1\n",
        "        t_dim = 1\n",
        "\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "        encoding_dim = img_dim + digit_dim + is_handwritten_dim + t_dim\n",
        "        self.fc1 = nn.Linear(encoding_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, img_dim)\n",
        "\n",
        "    def forward(self, x_t, digit, is_handwritten, t):\n",
        "        x_t = x_t.view(x_t.size(0), -1)\n",
        "        t = t.unsqueeze(1).float()\n",
        "        input = torch.cat([x_t, digit, is_handwritten, t], dim=1)\n",
        "        hidden = self.softplus(self.fc1(input))\n",
        "        eps_hat = self.fc2(hidden)\n",
        "        return eps_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward Process of Diffusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section implements the forward diffusion process given the original image x_0, the alphas schedule, and the number of time steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "T = 1000\n",
        "betas = torch.linspace(1e-4, 0.02, T)\n",
        "alphas = 1.0 - betas\n",
        "alpha_bars = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "def q_sample(x0, t, eps):\n",
        "    sqrt_ab = alpha_bars[t].sqrt().unsqueeze(1)\n",
        "    sqrt_1_ab = (1 - alpha_bars[t]).sqrt().unsqueeze(1)\n",
        "    return sqrt_ab * x0 + sqrt_1_ab * eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYlhi-IlpshP"
      },
      "source": [
        "## Listing 5.5: The causal model\n",
        "\n",
        "The `model` method implements the causal model. First it samples the latent variable Z, the digit variable, and the is_handwritten variable. These are passed to the decoder, which generates the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ndMbxPUfJlqu"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/local/scratch/a/jshreeku/miniconda3/envs/causal_ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "dist.enable_validation(False)    #A\n",
        "def model(self, data_size=1):    #B\n",
        "    pyro.module(\"decoder\", self.decoder)    #B\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    z_loc = torch.zeros(data_size, self.z_dim, **options)    #C\n",
        "    z_scale = torch.ones(data_size, self.z_dim, **options)    #C\n",
        "    # print(z_loc.shape, z_scale.shape)\n",
        "    z = pyro.sample(\"Z\", dist.Normal(z_loc, z_scale).to_event(1))    #C - we are defining the prior belief that latent styles are normally distributed around origin\n",
        "    # print(z.shape) # shape = (batch_size, z_dim)\n",
        "    p_digit = torch.ones(data_size, 10, **options)/10    #D - categorical RV - prior probability distribution for the 10 digits\n",
        "    digit = pyro.sample(    #D\n",
        "        \"digit\",    #D\n",
        "        dist.OneHotCategorical(p_digit)    #D\n",
        "    )    #D\n",
        "    p_is_handwritten = torch.ones(data_size, 1, **options)/2    #E - bernoulli RV - prior probability for MNIST vs TMNIST\n",
        "    is_handwritten = pyro.sample(    #E\n",
        "        \"is_handwritten\",    #E\n",
        "        dist.Bernoulli(p_is_handwritten).to_event(1)    #E\n",
        "    )    #E\n",
        "    print(is_handwritten.shape)\n",
        "    img_param = self.decoder(z, digit, is_handwritten)    #F - Each element of the vector corresponds to a Bernoulli parameter value for an image pixel.\n",
        "    img = pyro.sample(\"img\", dist.Bernoulli(img_param).to_event(1))  #G - samples to get a realization from the img_param distribution\n",
        "    return img, digit, is_handwritten\n",
        "#A Disabling distribution validation lets Pyro calculate loglikelihoods for pixels even though the pixels are not binary values.\n",
        "#B The model of a single image. Within the method we register the decoder, a PyTorch module, with Pyro. This lets Pyro know about the parameters inside of the decoder network.\n",
        "#C We model the joint probability of Z, digit, and is_handwritten sampling each from canonical distributions. We sample Z from a multivariate normal with location parameter z_loc (all zeros) and scale parameter z_scale (all ones).\n",
        "#D We also sample the digit from a one-hot categorical distribution. Equal probability is assigned to each digit.\n",
        "#E We similarly sample the is_handwritten variable from a Bernoulli.\n",
        "#F The decoder maps digit, is_handwritten, and Z to a probability parameter vector.\n",
        "#G That parameter vector is passed to the Bernoulli distribution, which models the pixel values in the data. The pixels are not technically Bernoulli binary variables, but we'll relax this assumption."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "\n",
        "def model(self, x0, data_size=1):\n",
        "    pyro.module(\"decoder\", self.decoder)\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    \n",
        "    # sample diffusion time\n",
        "    batched_probs = torch.ones(data_size, T, device=x0.device) / T\n",
        "    t = pyro.sample(\"t\", dist.Categorical(batched_probs))\n",
        "\n",
        "    # sample noise\n",
        "    eps = pyro.sample(\"eps\", dist.Normal(0, 1).expand(x0.shape).to_event(1))\n",
        "    \n",
        "    p_digit = torch.ones(data_size, 10, **options)/10    #D - categorical RV - prior probability distribution for the 10 digits\n",
        "    digit = pyro.sample(    #D\n",
        "        \"digit\",    #D\n",
        "        dist.OneHotCategorical(p_digit)    #D\n",
        "    )    #D\n",
        "    p_is_handwritten = torch.ones(data_size, 1, **options)/2    #E - bernoulli RV - prior probability for MNIST vs TMNIST\n",
        "    is_handwritten = pyro.sample(    #E\n",
        "        \"is_handwritten\",    #E\n",
        "        dist.Bernoulli(p_is_handwritten).to_event(1)    #E\n",
        "    )    #E\n",
        "\n",
        "    # forward diffusion (deterministic)\n",
        "    x_t = q_sample(x0, t, eps)\n",
        "\n",
        "    # predict noise\n",
        "    eps_hat = self.decoder(x_t, digit, is_handwritten, t)\n",
        "\n",
        "    eps_realization = pyro.sample(\"obs_eps\", dist.Normal(eps_hat, 1.0).to_event(1), obs=eps)\n",
        "\n",
        "    return x_t, eps_realization, digit, is_handwritten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49xn77L8qCmn"
      },
      "source": [
        "## Listing 5.6 Method for applying model to N images in data\n",
        "\n",
        "`training_model` extends `model` towards representing each image in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RQGgpOPSWOKa"
      },
      "outputs": [],
      "source": [
        "def training_model(self, img, digit, is_handwritten, batch_size):    #A\n",
        "    conditioned_on_data = pyro.condition(    #B\n",
        "        self.model,\n",
        "        data={\n",
        "            \"digit\": digit,\n",
        "            \"is_handwritten\": is_handwritten,\n",
        "            \"img\": img\n",
        "        }\n",
        "    )\n",
        "    with pyro.plate(\"data\", batch_size):    #C\n",
        "        img, digit, is_handwritten = conditioned_on_data(batch_size)\n",
        "    return img, digit, is_handwritten\n",
        "#A The model represents the data generating process for one image. The training_model applies that model to the N images in the training data.\n",
        "#B Now we condition the model on the evidence in the training data.\n",
        "#C This context manager represents the N-size plate representing repeating IID examples in the data in Figure 5.9. In this case, N is the batch size. It works like a for loop iterating over each data unit in the batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_model(self, img, digit, is_handwritten, T, batch_size):\n",
        "    # Condition the model on the labels provided by the dataset\n",
        "    conditioned_on_data = pyro.condition(\n",
        "        self.model,\n",
        "        data={\n",
        "            \"digit\": digit,\n",
        "            \"is_handwritten\": is_handwritten\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    # We wrap everything in the data plate\n",
        "    with pyro.plate(\"data\", batch_size):\n",
        "        # We pass 'img' (x0) into the model so it can be diffused\n",
        "        x_t, noise_t, digit, is_handwritten = conditioned_on_data(x0=img, data_size=batch_size)\n",
        "\n",
        "    return x_t, noise_t, digit, is_handwritten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia-xBsT6qioA"
      },
      "source": [
        "## Listing 5.7: Implement the encoder\n",
        "\n",
        "The encoder takes an image, the digit, and whether the variable is handwritten, and infers the latent representation Z."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UGGxsLWIWkvG"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):    #A\n",
        "    def __init__(self, z_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        img_dim = 28 * 28    #B\n",
        "        digit_dim = 10    #C\n",
        "        is_handwritten_dim = 1\n",
        "        self.softplus = nn.Softplus()    #D\n",
        "        input_dim = img_dim + digit_dim + is_handwritten_dim    #E\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)    #E\n",
        "        self.fc21 = nn.Linear(hidden_dim, z_dim)    #F\n",
        "        self.fc22 = nn.Linear(hidden_dim, z_dim)    #F\n",
        "\n",
        "    def forward(self, img, digit, is_handwritten):    #G\n",
        "        input = torch.cat([img, digit, is_handwritten], dim=1)    #H\n",
        "        hidden = self.softplus(self.fc1(input))    #I\n",
        "        z_loc = self.fc21(hidden)    #J\n",
        "        z_scale = torch.exp(self.fc22(hidden))    #J\n",
        "        return z_loc, z_scale\n",
        "#A The encoder is an instance of a Pytorch module.\n",
        "#B The input image is 28X28 = 784 pixels.\n",
        "#C The digit dimension is 10.\n",
        "#D In the encoder, we’ll only use the softplus transform (activation function).\n",
        "#E The linear transform fc1 combines with the softplus to map the 784 dimensional pixel vector, 10 dimensional digit label vector, and 2 dimensional is_handwritten vector to the hidden layer.\n",
        "#F The linear transforms fc21 and fc22 will combine with the softplus to map the hidden vector to Z’s vector space.\n",
        "#G Define the reverse computation from an observed X variable value to a latent Z variable value.\n",
        "#H Combine the image vector, digit label, and is-handwritten label into one input.\n",
        "#I Map the input to the hidden layer.\n",
        "#J The VAE framework will sample Z from a Normal distribution that approximates P(Z|img, digit, is_handwritten). The final transforms map the hidden layer to a location and scale parameter for that Normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is no encoder model for diffusion, the forward process is deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP920mmcqtsS"
      },
      "source": [
        "## Listing 5.8: The guide function\n",
        "\n",
        "`training_guide` contains the encoder. The purpose of `training_guide` is to approximate P(Z|image, digit, is_handwritten) during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xy2T3-CtZp5d"
      },
      "outputs": [],
      "source": [
        "def training_guide(self, img, digit, is_handwritten, batch_size):    #A\n",
        "    pyro.module(\"encoder\", self.encoder)    #B\n",
        "    options = dict(dtype=torch.float32, device=DEVICE_TYPE)\n",
        "    with pyro.plate(\"data\", batch_size):    #C\n",
        "        z_loc, z_scale = self.encoder(img, digit, is_handwritten)    #D\n",
        "        normal_dist = dist.Normal(z_loc, z_scale).to_event(1)    #D\n",
        "        z = pyro.sample(\"Z\", normal_dist)    #E\n",
        "#A training_guide is a method of the VAE which will use the encoder.\n",
        "#B Register the encoder so Pyro is aware of its weight parameters.\n",
        "#C This is the same plate context manager for iterating over the batch data that we see in the training_model.\n",
        "#D Use the encoder to map an image and its labels to parameters of a Normal distribution.\n",
        "#E Sample Z from that Normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def training_guide(self, img, digit, is_handwritten, T, batch_size):\n",
        "    batch_size = img.size(0)    \n",
        "    with pyro.plate(\"data\", batch_size):\n",
        "        # 1. We \"infer\" a random timestep for this training piece\n",
        "        batched_probs = torch.ones(batch_size, T, device=img.device) / T\n",
        "        t = pyro.sample(\"t\", dist.Categorical(batched_probs))       \n",
        "        # 2. We \"infer\" the exogenous noise.\n",
        "        # This acts as the 'Z' (latent variable) for this specific image.\n",
        "        \n",
        "        eps = pyro.sample(\"eps\", dist.Normal(0, 1).expand(img.shape).to_event(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bbit-PBlrayq"
      },
      "source": [
        "## Listing 5.9: The full VAE code\n",
        "\n",
        "Now we implement all the parts in the VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o5vOr1GXe_3P"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        z_dim=50,    #A\n",
        "        hidden_dim=400,    #B\n",
        "        use_cuda=USE_CUDA,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.z_dim = z_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.setup_networks()\n",
        "\n",
        "    def setup_networks(self):    #C\n",
        "        self.encoder = Encoder(self.z_dim, self.hidden_dim)\n",
        "        self.decoder = Decoder(self.z_dim, self.hidden_dim)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "\n",
        "    # Thus us where pyro gets integrated with pytorch\n",
        "    model = model    #D\n",
        "    training_model = training_model    #D\n",
        "    training_guide = training_guide    #D\n",
        "\n",
        "#A Setting the latent dimension to have a dimension of 50.\n",
        "#B Setting the hidden layers to have a dimension of 400.\n",
        "#C Setup the encoder and decoder.\n",
        "#D Adding in the methods for model, training_model, and training_guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Diffusion(nn.Module):\n",
        "    def __init__(self, T=1000, hidden_dim=400, use_cuda=USE_CUDA):\n",
        "        super().__init__()\n",
        "        self.use_cuda = use_cuda\n",
        "        self.T = T\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # 1. Setup the reverse-process network (Reverse Diffusion)\n",
        "        self.decoder = Decoder(hidden_dim)\n",
        "        \n",
        "        # 2. Setup fixed diffusion schedule (The Forward Process)\n",
        "        self.setup_schedule()\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def setup_schedule(self):\n",
        "        \"\"\"Precompute alpha-bars for the q_sample math.\"\"\"\n",
        "        self.betas = torch.linspace(1e-4, 0.02, self.T)\n",
        "        self.alphas = 1.0 - self.betas\n",
        "        # Register as buffer so it moves to GPU with the model\n",
        "        self.register_buffer('alpha_bars', torch.cumprod(self.alphas, dim=0))\n",
        "\n",
        "    def q_sample(self, x0, t, eps):\n",
        "        \"\"\"The forward process: x0 -> xt\"\"\"\n",
        "        sqrt_ab = self.alpha_bars[t].sqrt().unsqueeze(1)\n",
        "        sqrt_1_ab = (1 - self.alpha_bars[t]).sqrt().unsqueeze(1)\n",
        "        return sqrt_ab * x0 + sqrt_1_ab * eps\n",
        "\n",
        "    def model(self, x0, data_size=1):\n",
        "        pyro.module(\"decoder\", self.decoder)\n",
        "        device = x0.device\n",
        "        options = dict(dtype=torch.float32, device=device)\n",
        "        \n",
        "        # 1. Sample diffusion time (Batched)\n",
        "        batched_probs = torch.ones(data_size, self.T, device=device) / self.T\n",
        "        t = pyro.sample(\"t\", dist.Categorical(batched_probs))\n",
        "\n",
        "        # 2. Sample exogenous noise (The latent 'style' equivalent)\n",
        "        eps = pyro.sample(\"eps\", dist.Normal(0, 1).expand(x0.shape).to_event(1))\n",
        "        \n",
        "        # 3. Sample Causal Parents (Conditioned during training)\n",
        "        p_digit = torch.ones(data_size, 10, **options) / 10\n",
        "        digit = pyro.sample(\"digit\", dist.OneHotCategorical(p_digit))\n",
        "\n",
        "        p_is_hw = torch.ones(data_size, 1, **options) / 2\n",
        "        is_handwritten = pyro.sample(\"is_handwritten\", dist.Bernoulli(p_is_hw).to_event(1))\n",
        "\n",
        "        # 4. Forward diffusion (Causal path)\n",
        "        x_t = self.q_sample(x0, t, eps)\n",
        "\n",
        "        # 5. Predict noise (Mechanism)\n",
        "        eps_hat = self.decoder(x_t, digit, is_handwritten, t)\n",
        "\n",
        "        # 6. Observation (Log-likelihood matching VAE's Bernoulli line)\n",
        "        pyro.sample(\"obs_eps\", dist.Normal(eps_hat, 1.0).to_event(1), obs=eps)\n",
        "\n",
        "        return x_t, eps, digit, is_handwritten\n",
        "\n",
        "    def training_model(self, img, digit, is_handwritten, batch_size):\n",
        "        conditioned_on_data = pyro.condition(\n",
        "            self.model,\n",
        "            data={\"digit\": digit, \"is_handwritten\": is_handwritten}\n",
        "        )\n",
        "        with pyro.plate(\"data\", batch_size):\n",
        "            # Pass image as x0 directly\n",
        "            return conditioned_on_data(x0=img, data_size=batch_size)\n",
        "\n",
        "    def training_guide(self, img, digit, is_handwritten, batch_size):\n",
        "        \"\"\"The guide mirrors the sampling of latents (t and eps).\"\"\"\n",
        "        with pyro.plate(\"data\", batch_size):\n",
        "            batched_probs = torch.ones(batch_size, self.T, device=img.device) / self.T\n",
        "            pyro.sample(\"t\", dist.Categorical(batched_probs))\n",
        "            pyro.sample(\"eps\", dist.Normal(0, 1).expand(img.shape).to_event(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8_IhBKK0kpO"
      },
      "source": [
        "## Listing 5.10 Helper function for plotting images\n",
        "\n",
        "The following utility functions helps us visualize progress during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ohxiEjB30lHa"
      },
      "outputs": [],
      "source": [
        "def plot_image(img, title=None):    #A\n",
        "    fig = plt.figure()\n",
        "    plt.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.show()\n",
        "#A Helper function for plotting an image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTFLSyVW06tr"
      },
      "source": [
        "## Listing 5.11: Define a helper functions for reconstructing and viewing the images\n",
        "\n",
        "These additional utility functions help us selected and reshape images, as well as generate new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "QqBkhFAy066G"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def reconstruct_img(vae, img, digit, is_hw, use_cuda=USE_CUDA):    #A\n",
        "#     img = img.reshape(-1, 28 * 28)\n",
        "#     digit = F.one_hot(torch.tensor(digit), 10)\n",
        "#     is_hw = torch.tensor(is_hw).unsqueeze(0)\n",
        "#     if use_cuda:\n",
        "#         img = img.cuda()\n",
        "#         digit = digit.cuda()\n",
        "#         is_hw = is_hw.cuda()\n",
        "#     z_loc, z_scale = vae.encoder(img, digit, is_hw)\n",
        "#     z = dist.Normal(z_loc, z_scale).sample()\n",
        "#     img_expectation = vae.decoder(z, digit, is_hw)\n",
        "#     return img_expectation.squeeze().view(28, 28).detach()\n",
        "\n",
        "# def compare_images(img1, img2):    #B\n",
        "#     fig = plt.figure()\n",
        "#     ax0 = fig.add_subplot(121)\n",
        "#     plt.imshow(img1.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "#     plt.axis('off')\n",
        "#     plt.title('original')\n",
        "#     ax1 = fig.add_subplot(122)\n",
        "#     plt.imshow(img2.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "#     plt.axis('off')\n",
        "#     plt.title('reconstruction')\n",
        "#     plt.show()\n",
        "# #A Given an input image, \"reconstructs\" the image by passing through the encoder then through the decoder.\n",
        "# #B Plots two images side by side for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'USE_CUDA' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreconstruct_img_diffusion\u001b[39m(diffusion, img, digit, is_hw, t_val=\u001b[32m500\u001b[39m, use_cuda=\u001b[43mUSE_CUDA\u001b[49m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# 1. Prepare Inputs\u001b[39;00m\n\u001b[32m      3\u001b[39m     img = img.reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m) \u001b[38;5;66;03m# Batch size of 1\u001b[39;00m\n\u001b[32m      4\u001b[39m     digit_tensor = F.one_hot(torch.tensor([digit]), \u001b[32m10\u001b[39m).float()\n",
            "\u001b[31mNameError\u001b[39m: name 'USE_CUDA' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def reconstruct_img(diffusion, img, digit, is_hw, t_val=500, use_cuda=USE_CUDA):\n",
        "    # 1. Prepare Inputs\n",
        "    img = img.reshape(1, -1) # Batch size of 1\n",
        "    digit_tensor = F.one_hot(torch.tensor([digit]), 10).float()\n",
        "    is_hw_tensor = torch.tensor([[is_hw]]).float()\n",
        "    # For reconstruction, we pick a fixed t (e.g., mid-way through diffusion)\n",
        "    t_tensor = torch.tensor([t_val])\n",
        "    \n",
        "    if use_cuda:\n",
        "        img = img.cuda()\n",
        "        digit_tensor = digit_tensor.cuda()\n",
        "        is_hw_tensor = is_hw_tensor.cuda()\n",
        "        t_tensor = t_tensor.cuda()\n",
        "\n",
        "    # 2. Add Noise (The Forward Process)\n",
        "    # We generate a random noise epsilon to 'corrupt' the image\n",
        "    eps = torch.randn_like(img)\n",
        "    x_t = diffusion.q_sample(img, t_tensor, eps)\n",
        "\n",
        "    # 3. Predict the Noise (The Reverse Process)\n",
        "    # The decoder tries to 'see through' the noise\n",
        "    eps_hat = diffusion.decoder(x_t, digit_tensor, is_hw_tensor, t_tensor)\n",
        "\n",
        "    # 4. Mathematical Reconstruction\n",
        "    # Based on the DDPM formula: x0 = (x_t - sqrt(1-alpha_bar)*eps) / sqrt(alpha_bar)\n",
        "    # However, a simpler 'visual' reconstruction is just showing what the model \n",
        "    # thinks the noise was. Let's calculate the predicted x0:\n",
        "    \n",
        "    sqrt_alpha_bar = diffusion.alpha_bars[t_tensor].sqrt()\n",
        "    sqrt_one_minus_alpha_bar = (1 - diffusion.alpha_bars[t_tensor]).sqrt()\n",
        "    \n",
        "    img_reconstructed = (x_t - sqrt_one_minus_alpha_bar * eps_hat) / sqrt_alpha_bar\n",
        "\n",
        "    return img_reconstructed.squeeze().view(28, 28).detach()\n",
        "\n",
        "def compare_images(original, noisy, reconstruction):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    \n",
        "    images = [original, noisy, reconstruction]\n",
        "    titles = ['Original', 'Corrupted (t=500)', 'Reconstructed']\n",
        "    \n",
        "    for ax, img, title in zip(axes, images, titles):\n",
        "        ax.imshow(img.cpu(), cmap='Greys_r', interpolation='nearest')\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "        \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79MHVV7313mT"
      },
      "source": [
        "## Listing 5.12: Data processing helper functions for training\n",
        "\n",
        "Next, we'll create some helper functions for handling the data. We'll use `get_random_example` to grab random images from the dataset. `reshape_data` will convert an image and its labels into input for the encoder. We'll use `generate_data` and `generate_coded_data` will simulate an image from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fmSgUQrl1tFN"
      },
      "outputs": [],
      "source": [
        "# import torch.nn.functional as F\n",
        "\n",
        "# def get_random_example(loader):    #A\n",
        "#     random_idx = np.random.randint(0, len(loader.dataset))    #A\n",
        "#     img, digit, is_handwritten = loader.dataset[random_idx]    #A\n",
        "#     return img.squeeze(), digit, is_handwritten    #A\n",
        "\n",
        "# def reshape_data(img, digit, is_handwritten):    #B\n",
        "#     digit = F.one_hot(digit, 10).squeeze()    #B\n",
        "#     img = img.reshape(-1, 28*28)    #B\n",
        "#     return img, digit, is_handwritten    #B\n",
        "\n",
        "# def generate_coded_data(vae, use_cuda=USE_CUDA):    #C\n",
        "#     z_loc = torch.zeros(1, vae.z_dim)    #C\n",
        "#     z_scale = torch.ones(1, vae.z_dim)    #C\n",
        "#     z = dist.Normal(z_loc, z_scale).to_event(1).sample()    #C\n",
        "#     p_digit = torch.ones(1, 10)/10    #C\n",
        "#     digit = dist.OneHotCategorical(p_digit).sample()    #C\n",
        "#     p_is_handwritten = torch.ones(1, 1)/2    #C\n",
        "#     is_handwritten = dist.Bernoulli(p_is_handwritten).sample()    #C\n",
        "#     if use_cuda:    #C\n",
        "#         z = z.cuda()\n",
        "#         digit = digit.cuda()\n",
        "#         is_handwritten = is_handwritten.cuda()    #C\n",
        "#     img = vae.decoder(z, digit, is_handwritten)    #C\n",
        "#     return img, digit, is_handwritten    #C\n",
        "\n",
        "# def generate_data(vae, use_cuda=USE_CUDA):    #D\n",
        "#     img, digit, is_handwritten = generate_coded_data(vae, use_cuda)    #D\n",
        "#     img = img.squeeze().view(28, 28).detach()    #D\n",
        "#     digit = torch.argmax(digit, 1)    #D\n",
        "#     is_handwritten = torch.argmax(is_handwritten, 1)    #D\n",
        "#     return img, digit, is_handwritten    #D\n",
        "# #A Chose a random example from the dataset.\n",
        "# #B Reshape the data.\n",
        "# #C Generate data that is encoded.\n",
        "# #D Generate (unencoded) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m     img = img.reshape(-\u001b[32m1\u001b[39m, \u001b[32m28\u001b[39m*\u001b[32m28\u001b[39m)    \u001b[38;5;66;03m#B\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img, digit, is_handwritten    \u001b[38;5;66;03m#B\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;129m@torch\u001b[39m.no_grad()\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mp_sample\u001b[39m(diffusion, x_t, t, digit, is_hw):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    Reverse diffusion step: samples x_{t-1} given x_t.\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# 1. Predict the noise using the decoder\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def get_random_example(loader):    #A\n",
        "    random_idx = np.random.randint(0, len(loader.dataset))    #A\n",
        "    img, digit, is_handwritten = loader.dataset[random_idx]    #A\n",
        "    return img.squeeze(), digit, is_handwritten    #A\n",
        "\n",
        "def reshape_data(img, digit, is_handwritten):    #B\n",
        "    digit = F.one_hot(digit, 10).squeeze()    #B\n",
        "    img = img.reshape(-1, 28*28)    #B\n",
        "    return img, digit, is_handwritten    #B\n",
        "\n",
        "@torch.no_grad()\n",
        "def p_sample(diffusion, x_t, t, digit, is_hw):\n",
        "    \"\"\"\n",
        "    Reverse diffusion step: samples x_{t-1} given x_t.\n",
        "    \"\"\"\n",
        "    # 1. Predict the noise using the decoder\n",
        "    eps_hat = diffusion.decoder(x_t, digit, is_hw, t)\n",
        "\n",
        "    # 2. Calculate coefficients for the reverse step\n",
        "    # Based on the DDPM math: x_{t-1} = 1/sqrt(alpha) * (x_t - beta/sqrt(1-ab) * eps_hat)\n",
        "    beta_t = diffusion.betas[t]\n",
        "    sqrt_one_minus_alpha_bar_t = (1 - diffusion.alpha_bars[t]).sqrt()\n",
        "    sqrt_alpha_t = diffusion.alphas[t].sqrt()\n",
        "\n",
        "    # The predicted mean of x_{t-1}\n",
        "    mean = (1 / sqrt_alpha_t) * (x_t - (beta_t / sqrt_one_minus_alpha_bar_t) * eps_hat)\n",
        "\n",
        "    if t == 0:\n",
        "        return mean\n",
        "    else:\n",
        "        # Add a bit of 'fresh' noise (Langevin dynamics) to keep generation diverse\n",
        "        noise = torch.randn_like(x_t)\n",
        "        sigma_t = beta_t.sqrt() # Standard DDPM variance choice\n",
        "        return mean + sigma_t * noise\n",
        "    \n",
        "def generate_coded_data(diffusion, use_cuda=USE_CUDA):\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    # 1. Start with Pure Noise (The Diffusion 'Latent')\n",
        "    img_shape = (1, 28 * 28)\n",
        "    x = torch.randn(img_shape, device=device)\n",
        "\n",
        "    # 2. Sample Causal Parents (Digit and Style)\n",
        "    p_digit = torch.ones(1, 10, device=device) / 10\n",
        "    digit = dist.OneHotCategorical(p_digit).sample()\n",
        "    \n",
        "    p_is_hw = torch.ones(1, 1, device=device) / 2\n",
        "    is_handwritten = dist.Bernoulli(p_is_hw).sample()\n",
        "\n",
        "    # 3. The Reverse Loop (The 'Generation' work)\n",
        "    # We step from t=999 all the way down to t=0\n",
        "    for t_idx in reversed(range(diffusion.T)):\n",
        "        t_tensor = torch.tensor([t_idx], device=device)\n",
        "        x = p_sample(diffusion, x, t_tensor, digit, is_handwritten)\n",
        "\n",
        "    return x, digit, is_handwritten\n",
        "\n",
        "def generate_data(diffusion, use_cuda=USE_CUDA):\n",
        "    # This remains the wrapper that cleans up shapes for plotting\n",
        "    img, digit, is_handwritten = generate_coded_data(diffusion, use_cuda)\n",
        "    \n",
        "    img = img.squeeze().view(28, 28).detach()\n",
        "    digit = torch.argmax(digit, 1)\n",
        "    # is_handwritten for Bernoulli is usually just the value\n",
        "    is_handwritten = is_handwritten.squeeze().round().int() \n",
        "    \n",
        "    return img, digit, is_handwritten"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFp3sIBu2-eW"
      },
      "source": [
        "## Listing 5.13: Set up the training procedure\n",
        "\n",
        "Next we set up traing. The training objective `Trace_ELBO` simultaneously trains the parameters of the encoder and the decoder. It focuses on minimizing reconstruction error (how much information is lost when an image encoded, and then decoded once again) and [KL-divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the distribution modeled by the guide (the variational distribution) and the P(Z|image, is_handwritten, digit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "iEbyt3uo3Fhm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(50000, 787)\n"
          ]
        }
      ],
      "source": [
        "# from pyro.infer import SVI, Trace_ELBO\n",
        "# from pyro.optim import Adam\n",
        "\n",
        "# pyro.clear_param_store()    #A\n",
        "# vae = VAE()    #B\n",
        "# train_loader, test_loader = setup_dataloaders(batch_size=256)    #C\n",
        "# svi_adam = Adam({\"lr\": 1.0e-3})    #D\n",
        "# model = vae.training_model    #E\n",
        "# guide = vae.training_guide    #E\n",
        "# svi = SVI(model, guide, svi_adam, loss=Trace_ELBO())    #E\n",
        "# #A Clear any values of the parameters in the guide memory.\n",
        "# #B Initalize the VAE\n",
        "# #C Load the data\n",
        "# #D Initialize the optizer\n",
        "# #E Initialize the SVI loss calculator. Loss negative \"expected lower bound\" (ELBO)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import Adam\n",
        "\n",
        "# 1. Clear previous parameters\n",
        "pyro.clear_param_store()\n",
        "\n",
        "# 2. Initialize the Diffusion Class\n",
        "T_steps = 1000\n",
        "diffusion_model = Diffusion(T=T_steps, hidden_dim=400)\n",
        "train_loader, test_loader = setup_dataloaders(batch_size=256)\n",
        "\n",
        "# 3. Setup Optimizer\n",
        "# We use a slightly lower learning rate often preferred for Diffusion\n",
        "optimizer = Adam({\"lr\": 1.0e-3})\n",
        "\n",
        "# 4. Define Model and Guide\n",
        "# We point SVI to the 'training' versions we wrote\n",
        "model = diffusion_model.training_model\n",
        "guide = diffusion_model.training_guide\n",
        "\n",
        "# 5. Initialize SVI\n",
        "# Trace_ELBO works here because our 'model' has an 'obs=' statement \n",
        "# and our 'guide' samples the same latent variables (t, eps).\n",
        "svi = SVI(model, guide, optimizer, loss=Trace_ELBO())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klKFXtLpTOos"
      },
      "source": [
        "## Listing 5.14: Setting up a test evaluation procedure\n",
        "\n",
        "When training generative models, it is useful to setup a procedure that uses test data to evaluate how well training is progressing. You can include anything you think is useful to monitor during training. Here, I calculate and print the loss function on the test data, just to make sure test loss is progressively decreasing along with training loss (a flattening of test loss while training loss continued to decrease would indicate overfitting).\n",
        "\n",
        "But a more direct signal at how well our model is training is to generate and view images. In my test evaluation procedure, I produce two visualizations. First, I inspect how well it can reconstruct a random image from the test data. I pass the image through the encoder then through the decoder, creating a “reconstruction” of the image. Then I plot the original and reconstructed image side-by-side and compare them visually, looking to see that they are close to identical.\n",
        "\n",
        "Next, I visualize how well it is performing as an overall generative model by generating and plotting an image from scratch. I run this code once each time a certain number of epochs are run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XSviOogCTN2d"
      },
      "outputs": [],
      "source": [
        "# def test_epoch(vae, test_loader):\n",
        "#     epoch_loss_test = 0    #A\n",
        "#     for img, digit, is_hw in test_loader:    #A\n",
        "#         batch_size = img.shape[0]    #A\n",
        "#         if USE_CUDA:    #A\n",
        "#             img = img.cuda()    #A\n",
        "#             digit = digit.cuda()    #A\n",
        "#             is_hw = is_hw.cuda()    #A\n",
        "#         img, digit, is_hw = reshape_data(    #A\n",
        "#             img, digit, is_hw    #A\n",
        "#         )    #A\n",
        "#         epoch_loss_test += svi.evaluate_loss(    #A\n",
        "#             img, digit, is_hw, batch_size    #A\n",
        "#         )    #A\n",
        "#     test_size = len(test_loader.dataset)    #A\n",
        "#     avg_loss = epoch_loss_test/test_size    #A\n",
        "#     print(\"Epoch: {} avg. test loss: {}\".format(epoch, avg_loss))    #A\n",
        "#     print(\"Comparing a random test image to its reconstruction:\")    #B\n",
        "#     random_example = get_random_example(test_loader)    #B\n",
        "#     img_r, digit_r, is_hw_r = random_example    #B\n",
        "#     img_recon = reconstruct_img(vae, img_r, digit_r, is_hw_r)    #B\n",
        "#     compare_images(img_r, img_recon)    #B\n",
        "#     print(\"Generate a random image from the model:\")    #C\n",
        "#     img_gen, digit_gen, is_hw_gen = generate_data(vae)    #C\n",
        "#     plot_image(img_gen, \"Generated Image\")    #C\n",
        "#     print(\"Intended digit: \", int(digit_gen))    #C\n",
        "#     print(\"Intended as handwritten: \", bool(is_hw_gen == 1))    #C\n",
        "# #A Calculate and print test loss.\n",
        "# #B Compare a random test image to its reconstruction.\n",
        "# #C Generate a random image from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_epoch(diffusion, test_loader, epoch):\n",
        "    epoch_loss_test = 0\n",
        "    \n",
        "    # 1. Calculate Test Loss (Quantitative Check)\n",
        "    for img, digit, is_hw in test_loader:\n",
        "        batch_size = img.shape[0]\n",
        "        if USE_CUDA:\n",
        "            img, digit, is_hw = img.cuda(), digit.cuda(), is_hw.cuda()\n",
        "            \n",
        "        img, digit, is_hw = reshape_data(img, digit, is_hw)\n",
        "        \n",
        "        # evaluate_loss does everything step() does but WITHOUT updating weights\n",
        "        epoch_loss_test += svi.evaluate_loss(img, digit, is_hw, batch_size)\n",
        "        \n",
        "    test_size = len(test_loader.dataset)\n",
        "    avg_loss = epoch_loss_test / test_size\n",
        "    print(f\"Epoch: {epoch} avg. test loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # 2. Visualize Reconstruction (Qualitative Check: Denoising)\n",
        "    print(\"Comparing a random test image to its (one-step) reconstruction:\")\n",
        "    img_r, digit_r, is_hw_r = get_random_example(test_loader)\n",
        "    \n",
        "    # We use a mid-range t (e.g., 400) to see if it can recover from significant noise\n",
        "    t_test = 400 \n",
        "    img_recon = reconstruct_img(diffusion, img_r, digit_r, is_hw_r, t_val=t_test)\n",
        "    \n",
        "    # You might want to modify compare_images to show the original vs reconstruction\n",
        "    compare_images(img_r.view(28, 28), img_recon)\n",
        "\n",
        "    # 3. Generate New Image (Qualitative Check: Full Reverse Process)\n",
        "    print(\"Generate a brand new image by sampling from pure noise:\")\n",
        "    # This calls your generate_data function with the 1000-step loop\n",
        "    img_gen, digit_gen, is_hw_gen = generate_data(diffusion)\n",
        "    \n",
        "    plot_image(img_gen, f\"Generated: Digit {int(digit_gen)}\")\n",
        "    print(f\"Intended digit: {int(digit_gen)}\")\n",
        "    print(f\"Intended as handwritten: {bool(is_hw_gen == 1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmvAUfzZ3X4M"
      },
      "source": [
        "## Listing 5.15: Running training and plotting progress\n",
        "\n",
        "Finally, we run training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "krc0vcPv3Zqx",
        "outputId": "27747b85-41e6-4a95-abbe-287b9fcfdf59"
      },
      "outputs": [],
      "source": [
        "# NUM_EPOCHS = 2500\n",
        "# TEST_FREQUENCY = 10\n",
        "\n",
        "# train_loss = []\n",
        "# train_size = len(train_loader.dataset)\n",
        "\n",
        "# for epoch in range(0, NUM_EPOCHS+1):    #A\n",
        "#     loss = 0\n",
        "#     for img, digit, is_handwritten in train_loader:\n",
        "#         batch_size = img.shape[0]\n",
        "#         if USE_CUDA:\n",
        "#             img = img.cuda()\n",
        "#             digit = digit.cuda()\n",
        "#             is_handwritten = is_handwritten.cuda()\n",
        "#         img, digit, is_handwritten = reshape_data(\n",
        "#             img, digit, is_handwritten\n",
        "#         )\n",
        "#         loss += svi.step(    #B\n",
        "#             img, digit, is_handwritten, batch_size    #B\n",
        "#         )    #B\n",
        "#     avg_loss = loss / train_size\n",
        "#     print(\"Epoch: {} avgs training loss: {}\".format(epoch, loss))\n",
        "#     train_loss.append(avg_loss)\n",
        "#     if epoch % TEST_FREQUENCY == 0:    #C\n",
        "#         test_epoch(vae, test_loader)    #C\n",
        "# #A Run the training procedure for a certain number of epochs.\n",
        "# #B Run a training step on one batch in one epoch.\n",
        "# #C The test data evaluation procedure runs every 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 2500\n",
        "TEST_FREQUENCY = 10  # Note: Generation is slow, so you might increase this later\n",
        "\n",
        "train_loss = []\n",
        "train_size = len(train_loader.dataset)\n",
        "\n",
        "for epoch in range(0, NUM_EPOCHS + 1):\n",
        "    loss = 0\n",
        "    for img, digit, is_handwritten in train_loader:\n",
        "        batch_size = img.shape[0]\n",
        "        \n",
        "        if USE_CUDA:\n",
        "            img = img.cuda()\n",
        "            digit = digit.cuda()\n",
        "            is_handwritten = is_handwritten.cuda()\n",
        "            \n",
        "        # Re-use your existing reshape_data function\n",
        "        img, digit, is_handwritten = reshape_data(img, digit, is_handwritten)\n",
        "        \n",
        "        # SVI.step logic:\n",
        "        # This calls: training_model(img, digit, is_handwritten, batch_size)\n",
        "        # And: training_guide(img, digit, is_handwritten, batch_size)\n",
        "        loss += svi.step(img, digit, is_handwritten, batch_size)\n",
        "        \n",
        "    avg_loss = loss / train_size\n",
        "    train_loss.append(avg_loss)\n",
        "    \n",
        "    print(f\"Epoch: {epoch} | Avg Training Loss (ELBO): {avg_loss:.4f}\")\n",
        "\n",
        "    # Run the qualitative and quantitative test procedure\n",
        "    if epoch % TEST_FREQUENCY == 0:\n",
        "        # Note: We pass 'diffusion_model' (your class instance) instead of 'vae'\n",
        "        test_epoch(diffusion_model, test_loader, epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3m6t3b8G4Fg"
      },
      "source": [
        "We can continue to use `generate_data` to generate from the model once we've trained it. Finally, we can save the resulting model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxgJvMrecHJ5"
      },
      "outputs": [],
      "source": [
        "#torch.save(vae.state_dict(), 'mnist_tmnist_weights.pt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
